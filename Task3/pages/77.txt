<!DOCTYPE html>
<html lang="ru" data-vue-meta="%7B%22lang%22:%7B%22ssr%22:%22ru%22%7D%7D">
<head >
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0,viewport-fit=cover,maximum-scale=1,user-scalable=0">
  <meta name="referrer" content="unsafe-url">
  <title>Как воспитать GPT-3 модель в домашних условиях / Хабр</title>
  <style>
    /* cyrillic-ext */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveSxf6TF0.woff2) format('woff2');
      unicode-range: U+0460-052F, U+1C80-1C88, U+20B4, U+2DE0-2DFF, U+A640-A69F, U+FE2E-FE2F;
    }

    /* cyrillic */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveQhf6TF0.woff2) format('woff2');
      unicode-range: U+0400-045F, U+0490-0491, U+04B0-04B1, U+2116;
    }

    /* latin-ext */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveSBf6TF0.woff2) format('woff2');
      unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
    }

    /* latin */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveRhf6.woff2) format('woff2');
      unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
    }

    /* non-breaking hyphen */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/l/font?kit=KFOlCnqEu92Fr1MmEU9vBh0_IsHAlmrO6g&skey=ee881451c540fdec&v=v29) format('woff2');
      unicode-range: U+02011;
    }
  </style>
  <link rel="preload" href="https://assets.habr.com/habr-web/css/chunk-vendors.b6238726.css" as="style"><link rel="preload" href="https://assets.habr.com/habr-web/js/chunk-vendors.76685433.js" as="script"><link rel="preload" href="https://assets.habr.com/habr-web/css/app.59c6701f.css" as="style"><link rel="preload" href="https://assets.habr.com/habr-web/js/app.067cb56a.js" as="script"><link rel="preload" href="https://assets.habr.com/habr-web/js/7298.c8f1d73c.js" as="script">
  <link rel="stylesheet" href="https://assets.habr.com/habr-web/css/chunk-vendors.b6238726.css"><link rel="stylesheet" href="https://assets.habr.com/habr-web/css/app.59c6701f.css">
  <script>window.i18nFetch = new Promise((res, rej) => {
          const xhr = new XMLHttpRequest();
          xhr.open('GET', '/js/i18n/ru-compiled.bb54036e20d5f2f436c26e6c1769b6b8.json');
          xhr.responseType = 'json';
          xhr.onload = function(e) {
            if (this.status === 200) {
              res({ru: xhr.response});
            } else {
              rej(e);
            }
          };
          xhr.send();
        });</script>
  
  <script data-vue-meta="ssr" type="application/ld+json" data-vmid="ldjson-schema">{"@context":"http:\/\/schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/habr.com\/ru\/company\/neoflex\/blog\/722584\/"},"headline":"Как воспитать GPT-3 модель в домашних условиях","datePublished":"2023-03-15T16:10:28+03:00","dateModified":"2023-03-15T17:37:43+03:00","author":{"@type":"Person","name":"neoflex"},"publisher":{"@type":"Organization","name":"Habr","logo":{"@type":"ImageObject","url":"https:\/\/habrastorage.org\/webt\/a_\/lk\/9m\/a_lk9mjkccjox-zccjrpfolmkmq.png"}},"description":"Чат-бот ChatGPT наделал много шума: люди задают ему вопросы и удивляются точности ответов, студенты&nbsp;с его помощью пишут дипломные работы, а копирайтеры публикуют...","url":"https:\/\/habr.com\/ru\/company\/neoflex\/blog\/722584\/#post-content-body","about":["c_neoflex","h_machine_learning","f_develop"],"image":["https:\/\/habr.com\/share\/publication\/722584\/fb946f092cd29038a12ae4c19955245d\/","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/c90\/e79\/121\/c90e79121c6be5565f88ee6142bfbb1e.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/3ac\/3eb\/d07\/3ac3ebd0715af23417ed59494ead9d61.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/c5a\/434\/2bf\/c5a4342bf3b48d6b076f810f40ee76d4.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/cd9\/65f\/122\/cd965f12248197356e6c911ff2a5d028.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/21f\/99e\/019\/21f99e019601451d43a5b4a8207907fe.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/f8a\/e01\/0d7\/f8ae010d70d27c8754d42562f0a5a112.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/bd6\/82f\/df8\/bd682fdf853c1546488055f9afe606c6.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/dd4\/77c\/578\/dd477c57808073c4fc001dc9f4fd8101.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/dab\/b03\/6df\/dabb036df12a0f8f9d521b2f66bb947e.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/8a3\/8c3\/3d6\/8a38c33d6362b78738016fe49b59fbeb.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/296\/183\/1c5\/2961831c58cc711297ac1415e35cea76.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/c44\/0e0\/313\/c440e0313411d4b96a92bace0feebf55.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/4ea\/f1f\/860\/4eaf1f86037ed3530ee32f2c02858914.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/411\/ad0\/838\/411ad08383e96d9500db5d33e27bd961.png"]}</script>
  <script src="//www.googletagservices.com/tag/js/gpt.js" async></script>
  <style>.grecaptcha-badge{visibility: hidden;}</style>
  <meta name="habr-version" content="2.116.0">
  
  <meta data-vue-meta="ssr" property="fb:app_id" content="444736788986613"><meta data-vue-meta="ssr" property="fb:pages" content="472597926099084"><meta data-vue-meta="ssr" name="twitter:card" content="summary_large_image"><meta data-vue-meta="ssr" name="twitter:site" content="@habr_com"><meta data-vue-meta="ssr" property="og:site_name" content="Хабр" data-vmid="og:site_name"><meta data-vue-meta="ssr" property="og:title" content="Как воспитать GPT-3 модель в домашних условиях" data-vmid="og:title"><meta data-vue-meta="ssr" name="twitter:title" content="Как воспитать GPT-3 модель в домашних условиях" data-vmid="twitter:title"><meta data-vue-meta="ssr" name="aiturec:title" content="Как воспитать GPT-3 модель в домашних условиях" data-vmid="aiturec:title"><meta data-vue-meta="ssr" name="description" content="Чат-бот ChatGPT наделал много шума: люди задают ему вопросы и удивляются точности ответов, студенты&amp;nbsp;с его помощью пишут дипломные работы, а копирайтеры публикуют статьи с названием типа «Сможет..." data-vmid="description"><meta data-vue-meta="ssr" itemprop="description" content="Чат-бот ChatGPT наделал много шума: люди задают ему вопросы и удивляются точности ответов, студенты&amp;nbsp;с его помощью пишут дипломные работы, а копирайтеры публикуют статьи с названием типа «Сможет..." data-vmid="description:itemprop"><meta data-vue-meta="ssr" property="og:description" content="Чат-бот ChatGPT наделал много шума: люди задают ему вопросы и удивляются точности ответов, студенты&amp;nbsp;с его помощью пишут дипломные работы, а копирайтеры публикуют статьи с названием типа «Сможет..." data-vmid="og:description"><meta data-vue-meta="ssr" name="twitter:description" content="Чат-бот ChatGPT наделал много шума: люди задают ему вопросы и удивляются точности ответов, студенты&amp;nbsp;с его помощью пишут дипломные работы, а копирайтеры публикуют статьи с названием типа «Сможет..." data-vmid="twitter:description"><meta data-vue-meta="ssr" property="aiturec:description" content="Чат-бот ChatGPT наделал много шума: люди задают ему вопросы и удивляются точности ответов, студенты&amp;nbsp;с его помощью пишут дипломные работы, а копирайтеры публикуют статьи с названием типа «Сможет..." data-vmid="aiturec:description"><meta data-vue-meta="ssr" itemprop="image" content="https://habrastorage.org/getpro/habr/upload_files/c90/e79/121/c90e79121c6be5565f88ee6142bfbb1e.png" data-vmid="image:itemprop"><meta data-vue-meta="ssr" property="og:image" content="https://habrastorage.org/getpro/habr/upload_files/c90/e79/121/c90e79121c6be5565f88ee6142bfbb1e.png" data-vmid="og:image"><meta data-vue-meta="ssr" property="og:image:width" content="1200" data-vmid="og:image:width"><meta data-vue-meta="ssr" property="og:image:height" content="630" data-vmid="og:image:height"><meta data-vue-meta="ssr" property="aiturec:image" content="https://habrastorage.org/getpro/habr/upload_files/c90/e79/121/c90e79121c6be5565f88ee6142bfbb1e.png" data-vmid="aiturec:image"><meta data-vue-meta="ssr" name="twitter:image" content="https://habrastorage.org/getpro/habr/upload_files/c90/e79/121/c90e79121c6be5565f88ee6142bfbb1e.png" data-vmid="twitter:image"><meta data-vue-meta="ssr" property="vk:image" content="https://habrastorage.org/getpro/habr/upload_files/c90/e79/121/c90e79121c6be5565f88ee6142bfbb1e.png?format=vk" data-vmid="vk:image"><meta data-vue-meta="ssr" property="aiturec:item_id" content="722584" data-vmid="aiturec:item_id"><meta data-vue-meta="ssr" property="aiturec:datetime" content="2023-03-15T13:10:28.000Z" data-vmid="aiturec:datetime"><meta data-vue-meta="ssr" content="https://habr.com/ru/company/neoflex/blog/722584/" property="og:url" data-vmid="og:url"><meta data-vue-meta="ssr" property="og:type" content="article" data-vmid="og:type"><meta data-vue-meta="ssr" property="og:locale" content="ru_RU" data-vmid="og:locale"><meta data-vue-meta="ssr" name="keywords" content="gpt-3, chatgpt, quantization, lora, low-ranked adapters, fine-tuning, pytorch, gpt, gpt-j, text generation">
  <link data-vue-meta="ssr" href="https://habr.com/ru/rss/post/722584/?fl=ru" type="application/rss+xml" title="" rel="alternate" name="rss"><link data-vue-meta="ssr" href="https://habr.com/ru/company/neoflex/blog/722584/" rel="canonical" data-vmid="canonical"><link data-vue-meta="ssr" rel="image_src" href="https://habrastorage.org/getpro/habr/upload_files/c90/e79/121/c90e79121c6be5565f88ee6142bfbb1e.png" data-vmid="image:href"><link data-vue-meta="ssr" rel="amphtml" href="https://habr.com/ru/amp/post/722584/">
  <meta name="apple-mobile-web-app-status-bar-style" content="#303b44">
  <meta name="msapplication-TileColor" content="#629FBC">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="mobile-web-app-capable" content="yes">
  <link
    rel="shortcut icon"
    type="image/png"
    sizes="16x16"
    href="https://assets.habr.com/habr-web/img/favicons/favicon-16.png"
  >
  <link
    rel="shortcut icon"
    type="image/png"
    sizes="32x32"
    href="https://assets.habr.com/habr-web/img/favicons/favicon-32.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="76x76"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-76.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="120x120"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-120.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="152x152"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-152.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="180x180"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-180.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="256x256"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-256.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 320px) and (device-height: 568px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1136x640.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2436x1125.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1792x828.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_828x1792.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1334x750.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1242x2668.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2208x1242.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1125x2436.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1242x2208.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2732x2048.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2688x1242.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2224x1668.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_750x1334.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2048x2732.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2388x1668.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1668x2224.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 320px) and (device-height: 568px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_640x1136.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1668x2388.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2048x1536.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1536x2048.png"
  >
  <link
    rel="mask-icon"
    color="#77a2b6"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-120.svg"
  >
  <link
    crossorigin="use-credentials"
    href="/manifest.webmanifest"
    rel="manifest"
  >
  <script async src="https://unpkg.com/pwacompat" crossorigin="anonymous"></script>
  <script>window.yaContextCb = window.yaContextCb || []</script>
  <script src="https://yandex.ru/ads/system/context.js" async></script>
</head>
<body>


<div id="app" data-server-rendered="true" data-async-called="true"><div class="tm-layout__wrapper"><!----> <div></div> <div class="tm-feature tm-feature"><!----></div> <header class="tm-header"><div class="tm-page-width"><div class="tm-header__container"><div class="tm-header__burger-nav"><button type="button" class="tm-header__button tm-header__button_burger"><svg height="16" width="16" class="tm-svg-img tm-header__icon tm-header__icon-burger"><title>Меню</title> <use xlink:href="/img/megazord-v28.617e16ca..svg#header-burger"></use></svg></button></div> <span class="tm-header__logo-wrap"><a href="/ru/" class="tm-header__logo tm-header__logo_ru"><svg height="16" width="16" class="tm-svg-img tm-header__icon"><title>Хабр</title> <use xlink:href="/img/habr-logo-ru.svg#logo"></use></svg></a> <span class="tm-header__beta-sign" style="display:none;">β</span></span> <!----> <div class="tm-header-user-menu tm-header_user-menu"><a href="/ru/search/" class="tm-header-user-menu__item tm-header-user-menu__search"><svg height="24" width="24" class="tm-svg-img tm-header-user-menu__icon tm-header-user-menu__icon_search"><title>Поиск</title> <use xlink:href="/img/megazord-v28.617e16ca..svg#search"></use></svg></a> <!----> <!----> <!----> <div class="tm-header-user-menu__item"><button data-test-id="menu-toggle-guest" class="tm-header-user-menu__toggle"><svg height="24" width="24" class="tm-svg-img tm-header-user-menu__icon tm-header-user-menu__icon_white"><title>Профиль</title> <use xlink:href="/img/megazord-v28.617e16ca..svg#header-user"></use></svg></button> <!----></div> <!----></div></div></div></header> <div class="tm-layout"><div class="tm-page-progress-bar"></div> <!----> <!----> <div class="tm-page-width"></div> <main class="tm-layout__container"><div hl="ru" companyName="neoflex" data-async-called="true" class="tm-page"><div class="tm-page-width"><div class="tm-page__header"><!----></div> <div class="tm-page__wrapper"><div class="tm-page__main tm-page__main_has-sidebar"><div class="pull-down"><!----> <div class="pull-down__header" style="height:0px;"><div class="pull-down__content" style="bottom:10px;"><svg height="24" width="24" class="tm-svg-img pull-down__arrow"><title>Обновить</title> <use xlink:href="/img/megazord-v28.617e16ca..svg#pull-arrow"></use></svg></div></div> <div class="tm-article-presenter"> <div class="tm-article-presenter__body"><div class="tm-misprint-area"><div class="tm-misprint-area__wrapper"><article class="tm-article-presenter__content tm-article-presenter__content_narrow"><div class="tm-article-presenter__header"> <div class="tm-article-snippet tm-article-presenter__snippet tm-article-snippet"><div class="tm-article-snippet__meta-container"><div class="tm-article-snippet__meta"><span class="tm-user-info tm-article-snippet__author"><a href="/ru/users/neoflex/" title="neoflex" class="tm-user-info__userpic"><div class="tm-entity-image"><img alt="" height="32" src="//habrastorage.org/r/w48/getpro/habr/avatars/9f6/8f0/bda/9f68f0bda31b08e17c0d6a2397a2e4a1.png" width="32" class="tm-entity-image__pic"></div></a> <span class="tm-user-info__user"><a href="/ru/users/neoflex/" class="tm-user-info__username">
      neoflex
      <!----></a> <span class="tm-article-datetime-published"><time datetime="2023-03-15T13:10:28.000Z" title="2023-03-15, 16:10">4 часа назад</time></span></span></span></div> <!----></div> <h1 lang="ru" class="tm-article-snippet__title tm-article-snippet__title_h1"><span>Как воспитать GPT-3 модель в домашних условиях</span></h1> <div class="tm-article-snippet__stats"><div class="tm-article-complexity tm-article-complexity_complexity-medium"><span class="tm-svg-icon__wrapper tm-article-complexity__icon"><svg height="24" width="24" class="tm-svg-img tm-svg-icon"><title>Уровень сложности</title> <use xlink:href="/img/megazord-v28.617e16ca..svg#complexity-medium"></use></svg></span> <span class="tm-article-complexity__label">
    Средний
  </span></div> <div class="tm-article-reading-time"><span class="tm-svg-icon__wrapper tm-article-reading-time__icon"><svg height="24" width="24" class="tm-svg-img tm-svg-icon"><title>Время на прочтение</title> <use xlink:href="/img/megazord-v28.617e16ca..svg#clock"></use></svg></span> <span class="tm-article-reading-time__label">
    10 мин
  </span></div> <span class="tm-icon-counter tm-data-icons__item"><svg height="24" width="24" class="tm-svg-img tm-icon-counter__icon"><title>Количество просмотров</title> <use xlink:href="/img/megazord-v28.617e16ca..svg#counter-views"></use></svg> <span class="tm-icon-counter__value">2.1K</span></span></div> <div class="tm-article-snippet__hubs-container"><div class="tm-article-snippet__hubs"><span class="tm-article-snippet__hubs-item"><a href="/ru/company/neoflex/blog/" class="tm-article-snippet__hubs-item-link router-link-active"><span>Блог компании Neoflex</span> <!----></a></span><span class="tm-article-snippet__hubs-item"><a href="/ru/hub/machine_learning/" class="tm-article-snippet__hubs-item-link"><span>Машинное обучение</span> <span title="Профильный хаб" class="tm-article-snippet__profiled-hub">*</span></a></span></div></div> <div class="tm-article-snippet__labels-container"><div class="tm-article-snippet__labels"><div class="tm-article-snippet__label tm-article-snippet__label_variant-review"><span>
          Обзор
        </span></div> </div></div> <!----> <!----></div></div> <!----> <div data-gallery-root="" lang="ru" class="tm-article-body"><div></div> <div id="post-content-body"><div><div class="article-formatted-body article-formatted-body article-formatted-body_version-2"><div xmlns="http://www.w3.org/1999/xhtml"><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/c90/e79/121/c90e79121c6be5565f88ee6142bfbb1e.png" width="780" height="440" data-src="https://habrastorage.org/getpro/habr/upload_files/c90/e79/121/c90e79121c6be5565f88ee6142bfbb1e.png"/></figure><p>Чат-бот ChatGPT наделал много шума: люди задают ему вопросы и удивляются точности ответов, студенты с его помощью пишут дипломные работы, а копирайтеры публикуют статьи с названием типа «Сможет ли ChatGPT заменить копирайтеров?». </p><p>Мы решили проверить технологию, на которой основан ChatGPT, посмотреть актуальное состояние open-source GPT-3-like моделей и ответить на вопрос — можно ли обучить GPT-3-like модель в домашних условиях?</p><p>Для эксперимента выбрали GPT-J и не самый мощный ПК с видеокартой Nvidia GTX 1080TI с 11 GB VRAM. Оказалось, что этого достаточно не только, чтобы загрузить модель, но и дообучить ее (fine-tune). Рассказываем — как мы это сделали.</p><h3>Почему именно GPT-J </h3><p>GPT (англ. Generative Pre-trained Transformer — генеративный предобученный трансформер) — это гигантская нейронная сеть, которая обучена на огромном количестве данных предсказывать следующее за последовательностью слово. GPT моделирует естественные языки, помнит контекст и генерирует тексты. Для тех, кто хочет узнать побольше про архитектуру GPT, мы оставим ссылки [<span><u>1</u></span>] и [<span><u>15</u></span>].</p><p>Понять преимущества модели GPT-J от команды EleutherAI проще всего, если взглянуть на таблицу, которую привели сами разработчики. Мы покажем ее в сокращенном виде. Полную версию можно посмотреть по ссылке [<span><u>2</u></span>].</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/3ac/3eb/d07/3ac3ebd0715af23417ed59494ead9d61.png" alt="Сравнение GPT-like моделей. " title="Сравнение GPT-like моделей. " width="846" height="525" data-src="https://habrastorage.org/getpro/habr/upload_files/3ac/3eb/d07/3ac3ebd0715af23417ed59494ead9d61.png"/><div><figcaption>Сравнение GPT-like моделей. </figcaption></div></figure><p>Насколько хороша модель, можно увидеть по значениям бенчмарков:</p><ul><li><p><strong>LAMBADA </strong>— это датасет, который используется как бенчмарк для предсказания слова по широкому контексту. Для этого бенчмарка в таблице приведены две метрики: перплексия <strong>PPl </strong>(меньше — лучше) и точность <strong>Acc </strong>(выше — лучше);</p></li><li><p><strong>Winorgande </strong>— это датасет с текстовыми задачами и вариантами ответов;</p></li><li><p><strong>Hellaswag </strong>— датасет с текстовыми задачами на здравый смысл;</p></li><li><p><strong>PIQA </strong>— также бенчмарк для оценки здравого смысла – нацелен на оценку физических знаний модели.</p></li></ul><p>По показателям видно, что среди public-моделей GPT-J выглядит лучше всех. А в сравнении с моделями от OpenAI GPT-J показывает сопоставимые результаты с моделью Curie и уступает лишь гигантской Davinci. Объем тренировочного датасета GPT-J также внушителен — 825 GB [<span><u>4</u></span>], а количество параметров (весов) модели ~ 6 миллиардов. </p><p>В качестве основного стека для дообучения GPT-J мы выбрали Python, transformers и pytorch lightning. GPT-J и другие достижения open source-сообщества можно найти на Hugging Face Hub [<span><u>5</u></span>]. </p><h3>8 bit GPT-J</h3><p>Модель от EleutherAI с float16 параметрами требует около 21 GB VRAM — а это для нас проблема. Чтобы как-то уместить этого монстра на наш ПК, мы можем прибегнуть к квантизации [<span><u>6</u></span>, <span><u>7</u></span>] — хитрым способом сопоставить оригинальные float16 параметры модели с int8 параметрами и уменьшить объем занимаемой памяти в два раза. Кажется, что будет потеря в точности, но судя по результатам сравнения GPT-J и GPT-J-8bit, разница в точности этих моделей в пределах погрешности [<span><u>8</u></span>].</p><p>Квантизованная модель GPT-J [<span><u>3</u></span>] доступна благодаря ребятам из <span><u>Training Transformers Together</u></span> [<span><u>9</u></span>] и занимает примерно 6 GB на GTX 1080 TI. Давайте с ней поздороваемся.</p><figure class="bordered full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/c5a/434/2bf/c5a4342bf3b48d6b076f810f40ee76d4.png" width="597" height="289" data-src="https://habrastorage.org/getpro/habr/upload_files/c5a/434/2bf/c5a4342bf3b48d6b076f810f40ee76d4.png"/></figure><p>Первым делом нужно разобраться, что принимает и возвращает модель — input и output соответственно, и какая у этого размерность. Модель принимает последовательность слов и возвращает вероятности следующего слова для каждого из последовательности. GPT очень внимательный (привет, attention!) слушатель, который, услышав слово, сразу же пытается предсказать следующее.  </p><h3>Input</h3><p>Так выглядят входные данные:</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/cd9/65f/122/cd965f12248197356e6c911ff2a5d028.png" width="890" height="229" data-src="https://habrastorage.org/getpro/habr/upload_files/cd9/65f/122/cd965f12248197356e6c911ff2a5d028.png"/></figure><p>Модель принимает вектор токенов (<strong>input_ids</strong>) и <strong>attention mask</strong>. Оба элемента с максимальной длиной 2048. </p><ul><li><p><strong>Вектор токенов</strong> состоит из целых чисел: id токенов из словаря модели. Словарь модели содержит 50257 токенов; </p></li><li><p><strong>Attention mask</strong>: вектор с нулями и единицами, задача которого — сообщить модели – на какие токены обращать внимание (1), а на какие — нет (0).</p></li></ul><p>Проще говоря, нам нужно общаться с GPT словами, которые уже есть в его словаре, а также акцентировать внимание на важных для беседы словах. Например, есть несколько случаев, когда мы используем 0 в attention mask. Во-первых, мы используем 0 для паддинг-токенов, когда собираем сэмплы в батч (порцию для обучения) и нам нужно сделать паддинг, чтобы длины сэмплов совпадали. Во-вторых, в процессе обучения мы можем заставить модель не обращать внимания на некоторые токены в сэмпле (например, на те, которые мы хотим предсказать). </p><p>На иллюстрации выше видно, как sample_text превращается в input_ids и attention_mask.</p><h3>Output </h3><p>Внимательный GPT постоянно пытается предсказать следующее слово, но кроме внимательности мы еще можем рассчитывать на его хорошую память. Модель по умолчанию возвращает вероятности следующих токенов (<strong>logits</strong>) и внутреннюю память модели (<strong>past_key_values</strong>). Так выглядят output нашей модели:</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/21f/99e/019/21f99e019601451d43a5b4a8207907fe.png" width="859" height="558" data-src="https://habrastorage.org/getpro/habr/upload_files/21f/99e/019/21f99e019601451d43a5b4a8207907fe.png"/></figure><p>Размер тензора <strong>logits </strong>— (batch_size, sample_len, vocab_size). Наш исходный сэмпл 'Hello, GPT-J! How are you?' содержит 12 токенов, поэтому logits имеет форму (1, 12, 50400). </p><p>Для каждого токена в исходном сэмпле мы получаем вероятности для всех токенов в словаре модели. При генерации текста нас интересует только вероятность для последнего токена. В дообучении модели могут участвовать все или только избранные вероятности для входной последовательности. </p><ul><li><p><strong>past_key_value</strong> — это и есть память нашей модели. Размер тензора: (n_layers, key_value, batch, n_attention_heads, sample_len, head_embedding_dimension);</p></li><li><p><strong>n_layers </strong>— это количество слоев GPT-J;</p></li><li><p><strong>key_value</strong> — кортеж из ключей и значений в контексте механизма внимания (Attention) [<span><u>10</u></span>];</p></li><li><p><strong>batch</strong> — размер батча; </p></li><li><p><strong>n_attention_heads</strong> — количество голов attention;</p></li><li><p><strong>sample_len </strong>— длина сэмпла;</p></li><li><p><strong>head_embedding_dimension</strong> — внутренний размер attention head;</p></li><li><p><strong>n_layers, key_value, n_attention_heads, head_embedding_dimension</strong> — размерности, которые относятся к конфигурации модели.</p></li></ul><h3>Тестовая задача</h3><p>Представим, что мы хотим научить GPT модерировать чаты — модель будет классифицировать сообщения на 3 класса: hate (содержащее ненависть), offensive (содержащее угрозу), neutral (нейтральное). </p><p>Для этого будем использовать Hate Speech and Offensive Language Dataset [<span><u>11</u></span>]. Поскольку GPT уже много знает о языке и словах, мы уменьшим количество тренировочных данных и возьмем случайным образом лишь 1000 примеров, предварительно сбалансировав классы. Для валидации выберем 200 сбалансированных примеров. Нам предстоит обучить GPT-J-8bit генерировать нужную метку класса.</p><h4>Как подготовить данные?</h4><p>Когда мы хотим чему-то научить GPT, нам нужно сказать ему начало фразы, а затем оценить его вариант завершения фразы, поэтому тренировочный сэмпл содержит две части — затравку и завершение.</p><p>Подготовка входного сэмпла играет важную роль для обучения модели. В тренировочный сэмпл, кроме самого текста, который подлежит классификации, можно добавить дополнительную информацию: начиная от специальных разделителей блоков сэмпла и заканчивая целыми инструкциями. К важному можно отнести специальные токены, которые отделяют затравку от завершения (то есть от того, чему мы хотим обучить модель). С помощью токенов-разделителей мы как бы указываем –  когда ждем от нашего GPT выполнения желаемого.  </p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/f8a/e01/0d7/f8ae010d70d27c8754d42562f0a5a112.png" width="874" height="209" data-src="https://habrastorage.org/getpro/habr/upload_files/f8a/e01/0d7/f8ae010d70d27c8754d42562f0a5a112.png"/></figure><p>Нам удалось добиться одинаково хорошей точности как в случае с затравками-инструкциями, так и в случае затравок, состоящих только из целевого текста с разделителем. </p><p>Единственный минус инструкций состоит в том, что входные сэмплы получаются длиннее. Но зато, благодаря инструкциям, модель лучше научится понимать – чего мы от нее хотим: GPT видит инструкцию и, соотнося ее с результатом, понимает, что все варианты ответов перечислены в инструкции и ей нужно только выбрать правильный. Это дает возможность научить GPT решать другие классы задач, которые могут быть сформулированы в рамках той же структуры инструкций. </p><h4>Что обучать? (Low-Rank Adapters)</h4><p>В GPT-J-8bit параметры квантизованы. Тренировка квантизованных целочисленных параметров привычными алгоритмами не представляется разумным подходом хотя бы потому, что область значений функции потерь cross entropy loss лежит в [0, 1]. Но даже квантизация не избавляет нас от тренировки огромного количества параметров и затрат на вычисления.</p><p>Авторы статьи <em>LoRA: Low-Rank Adaption of Large Language Models</em> [<span><u>12</u></span>] предлагают очень интересный и эффективный способ дообучения огромных моделей. Зачем тренировать все параметры модели? Давайте заморозим все слои и к каждому добавим тренируемый адаптер с низкой размерностью. Адаптер представляет собой два линейных слоя A и B размера (d, r) и (r, d). И вместо тренировки «родного» слоя модели W размера (d,d) (а у  GPT-J d равен 4096) предлагается тренировать две матрицы поменьше. Авторы LoRA демонстрируют, что оптимальный r равен 2, чем оправдывают название своего подхода Low-Rank Adaption. Его логика отображена на схеме:</p><figure class="bordered "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/bd6/82f/df8/bd682fdf853c1546488055f9afe606c6.png" alt="LoRA" title="LoRA" width="292" height="255" data-src="https://habrastorage.org/getpro/habr/upload_files/bd6/82f/df8/bd682fdf853c1546488055f9afe606c6.png"/><div><figcaption>LoRA</figcaption></div></figure><h4>Как обучать? (Adam8bit)</h4><p>Загрузить модель в оперативную память видеокарты — это еще не приручить ее. Для дообучения GPT стандартными техниками требуется еще одна видеокарта, но мы обойдемся без нее и воспользуемся квантизованным оптимизатором. </p><p>Стандартным инструментом обучения параметров модели является оптимизатор Adam. Нам же нужен какой-то экономный аналог. В статье <em>8-Bit Optimizers via Block-wise Quantization</em> [<span><u>13</u></span>] авторы предлагают квантизовать оптимизатор, в частности, его состояния, которые включают статистику по градиентам (поправкам) для параметров модели. Более того, предлагается блочная (block-wise) квантизация, которая может выполняться параллельно. Картинка из оригинальной статьи прекрасно иллюстрирует суть этого:</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/dd4/77c/578/dd477c57808073c4fc001dc9f4fd8101.png" width="667" height="319" data-src="https://habrastorage.org/getpro/habr/upload_files/dd4/77c/578/dd477c57808073c4fc001dc9f4fd8101.png"/></figure><p>Спасибо авторам за то, что они также опубликовали репозиторий bitsandbytes [<span><u>14</u></span>] с реализацией их подхода. </p><h4>Последние приготовления и обучение</h4><p>Мы описали ключевые способы, которые помогут нам приручить GPT-J. Но для того, чтобы научить GPT делать то, что нам нужно, необходимо как-то указывать ему на его ошибки. Мы используем <strong>loss mask</strong> в своем коде для того, чтобы взять из предсказаний модели только то, что должно быть сгенерировано, и сделать обратное распространение ошибки только по этим токенам. Так мы не будем считать ошибку на затравке, а посчитаем loss только на продолжении. И модель будет лучше понимать, что мы от нее хотим. Также если мы решаем задачу классификации, можно подсчитать точность предсказаний — это и будет метрикой оценки модели.</p><p>Вот так выглядит шаг обучения модели:</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/dab/b03/6df/dabb036df12a0f8f9d521b2f66bb947e.png" width="560" height="392" data-src="https://habrastorage.org/getpro/habr/upload_files/dab/b03/6df/dabb036df12a0f8f9d521b2f66bb947e.png"/></figure><p>В зависимости от длины сэмплов и объема памяти видеокарты мы можем использовать батчи. Нам потребуется привести к одинаковой длине все сэмплы — сделать паддинг коротких сэмплов и обрезать длинные. Батчинг поможет ускорить процесс обучения. </p><p>После того, как архитектура GPT-J с адаптерами собрана, данные для обучения загружены и предобработаны, а оптимизатор Adam8bit наготове. Мы можем приступать к fine-tuning’у и валидации. </p><p>Будем обучать модель на протяжении трёх эпох. Также сравним четыре подхода:</p><ul><li><p>Обучение 1D параметров модели (те слои, где использование адаптера бессмысленно) — 861 K обучаемых параметров;</p></li><li><p>Адаптеры только для attention слоев — 2.7 M обучаемых параметров;</p></li><li><p>Адаптеры для всех слоев — 5.2 М обучаемых параметров;</p></li><li><p>Few-shot — без обучения.</p></li></ul><p>Мы имеем дело с «умной» моделью, которая много знает о языке, поэтому можем вообще отказаться от тренировочных данных и положиться на саму модель. Few-shot подход, описанный в статье от команды OpenAI [<span><u>15</u></span>], предлагает использовать несколько тренировочных сэмплов в затравке, получая в результате завершения для новых случаев. </p><h4>Результаты </h4><p>Для начала взглянем на отчет [<span><u>16</u></span>] по обучению для нашей GPT-J. График с <strong>loss </strong>нам нужен, чтобы убедиться, что модель обучается и loss уменьшается со временем от эпохи к эпохе. А от <strong>accuracy </strong>мы ожидаем, что она будет расти.</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/8a3/8c3/3d6/8a38c33d6362b78738016fe49b59fbeb.png" alt="GPT-J fine-tuning report " title="GPT-J fine-tuning report " width="764" height="602" data-src="https://habrastorage.org/getpro/habr/upload_files/8a3/8c3/3d6/8a38c33d6362b78738016fe49b59fbeb.png"/><div><figcaption>GPT-J fine-tuning report </figcaption></div></figure><p>Теперь испытаем модели OpenAI через API в идентичных условиях. Вот отчет по fine-tuning’у GPT-3 Ada и GPT-3 Davinci [<span><u>17</u></span>]. Предположительно, значение loss отличаются на порядок от нашей модели из-за различия функций потерь. В нашем подходе мы использовали стандартную cross entropy. </p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/296/183/1c5/2961831c58cc711297ac1415e35cea76.png" alt="OpenAI’s models fine-tuning report " title="OpenAI’s models fine-tuning report " width="720" height="569" data-src="https://habrastorage.org/getpro/habr/upload_files/296/183/1c5/2961831c58cc711297ac1415e35cea76.png"/><div><figcaption>OpenAI’s models fine-tuning report </figcaption></div></figure><p>Взглянем на таблицу точности предсказаний всех опробованных моделей и подходов:</p><figure class=""><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/c44/0e0/313/c440e0313411d4b96a92bace0feebf55.png" alt="Сравнение GPT-J и моделей OpenAI" title="Сравнение GPT-J и моделей OpenAI" width="514" height="235" data-src="https://habrastorage.org/getpro/habr/upload_files/c44/0e0/313/c440e0313411d4b96a92bace0feebf55.png"/><div><figcaption>Сравнение GPT-J и моделей OpenAI</figcaption></div></figure><p>Нам удалось превзойти точность моделей от OpenAI в тестовой задаче модерации при идентичных условиях. Адаптеры предлагают наилучшую точность, и чем их больше, тем точнее предсказания модели. Few-shot подход оставляет желать лучшего. А ChatGPT (GPT-3.5-turbo) дает весьма посредственную точность, хотя и лучше, чем GPT-J без дообучения.</p><p>Мы смогли обучить  GPT-J в домашних условиях, причем так, что он не уступает разработкам больших компаний. Однако важно отметить, что если дообучать OpenAI GPT-3 модели предложенным в документации API способом — без инструкций, только сырой текст с разделителем — то Davinci демонстрирует идентичную точность — 84 %. С другой стороны, использование дообученных моделей OpenAI будет стоить денег: например, за генерацию приблизительно двух печатных страниц текста мы заплатим 12 центов для самой крутой модели, причем в стоимость входят и затравки.</p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/4ea/f1f/860/4eaf1f86037ed3530ee32f2c02858914.png" alt="Цена генерации дообученных моделей" title="Цена генерации дообученных моделей" width="611" height="238" data-src="https://habrastorage.org/getpro/habr/upload_files/4ea/f1f/860/4eaf1f86037ed3530ee32f2c02858914.png"/><div><figcaption>Цена генерации дообученных моделей</figcaption></div></figure><h3>Преимущества инструкций в затравках</h3><p>Чтобы подтвердить, что решение помещать в затравки инструкции действительно имеет преимущества, мы провели еще один эксперимент — предложили модели выбрать категорию для заголовков новостей с сайта BBC.</p><p>Дообученная модель показывает, что благодаря инструкциям в затравках она научилась понимать, что мы от нее хотим, без дополнительного обучения на новых категориях. </p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/411/ad0/838/411ad08383e96d9500db5d33e27bd961.png" alt="Классификация заголовков новостей" title="Классификация заголовков новостей" width="1203" height="524" data-src="https://habrastorage.org/getpro/habr/upload_files/411/ad0/838/411ad08383e96d9500db5d33e27bd961.png"/><div><figcaption>Классификация заголовков новостей</figcaption></div></figure><h3>Заключение</h3><p>Когда мы заканчивали исследование, OpenAI предоставили доступ к API ChatGPT — gpt-3.5-turbo. Завышенные ожидания от технологии вызвали опасение, что gpt-3.5-turbo может обесценить это исследование. Но, как оказалось, чуда не произошло — результаты работы этой модели вы могли видеть в таблице выше. ChatGPT не может решить конкретную прикладную задачу так же хорошо, как дообученная модель.</p><p>Нам до сих пор нужно быть нянькой для искусственного интеллекта: готовить данные и придумывать все более эффективные способы дообучения. Одомашнить многообещающую открытую GPT помогли концепции тренируемых адаптеров и квантизуемого оптимизатора. Мы также предлагаем ознакомиться с репозиторием [<span><u>18</u></span>], который воспроизводит все полученные здесь результаты.  </p><p><strong>Ссылки</strong></p><ol><li><p><span><u>Building a GPT-like Model from Scratch with Detailed Theory and Code Implementation / Хабр (habr.com)</u></span></p></li><li><p><span><u>EleutherAI/gpt-j-6B · Hugging Face</u></span></p></li><li><p><span><u>hivemind/gpt-j-6B-8bit · Hugging Face</u></span></p></li><li><p><span><u>The Pile (eleuther.ai)</u></span></p></li><li><p><span><u>Models - Hugging Face</u></span></p></li><li><p><span><u>How to accelerate and compress neural networks with quantization | by Tivadar Danka | Towards Data Science</u></span></p></li><li><p><span><u>Achieving FP32 Accuracy for INT8 Inference Using Quantization Aware Training with NVIDIA TensorRT | NVIDIA Technical Blog</u></span></p></li><li><p><span><u>Jupyter Notebook Viewer (nbviewer.org)</u></span></p></li><li><p>https://training-transformers-together.github.io</p></li><li><p><span><u>[1706.03762] Attention Is All You Need (arxiv.org)</u></span></p></li><li><p><span><u>Hate Speech and Offensive Language Dataset | Kaggle</u></span></p></li><li><p><span><u>[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models (arxiv.org)</u></span></p></li><li><p><span><u>[2110.02861] 8-bit Optimizers via Block-wise Quantization (arxiv.org)</u></span></p></li><li><p><span><u>TimDettmers/bitsandbytes: 8-bit CUDA functions for PyTorch (github.com)</u></span></p></li><li><p><span><u>[2005.14165] Language Models are Few-Shot Learners (arxiv.org)</u></span></p></li><li><p><span><u>https://api.wandb.ai/links/vetka925/d7bb74kg</u></span> </p></li><li><p>https://api.wandb.ai/links/vetka925/krtz93ul</p></li><li><p><span><u>vetka925/gpt-j-8bit-lightning-finetune (github.com)</u></span></p></li></ol><p>Автор статьи: Виталий Гречачин, Data Scientist в компании Neoflex.</p><p></p></div></div></div> <!----> <!----></div> <!----> <!----></div> <!----> <div class="tm-article-presenter__meta"><div class="tm-separated-list tm-article-presenter__meta-list"><span class="tm-separated-list__title">Теги:</span> <ul class="tm-separated-list__list"><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bgpt-3%5D" class="tm-tags-list__link">gpt-3</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bchatgpt%5D" class="tm-tags-list__link">chatgpt</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bquantization%5D" class="tm-tags-list__link">quantization</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Blora%5D" class="tm-tags-list__link">lora</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Blow-ranked%20adapters%5D" class="tm-tags-list__link">low-ranked adapters</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bfine-tuning%5D" class="tm-tags-list__link">fine-tuning</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bpytorch%5D" class="tm-tags-list__link">pytorch</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bgpt%5D" class="tm-tags-list__link">gpt</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bgpt-j%5D" class="tm-tags-list__link">gpt-j</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Btext%20generation%5D" class="tm-tags-list__link">text generation</a></li></ul></div> <div class="tm-separated-list tm-article-presenter__meta-list"><span class="tm-separated-list__title">Хабы:</span> <ul class="tm-separated-list__list"><li class="tm-separated-list__item"><a href="/ru/company/neoflex/blog/" class="tm-hubs-list__link router-link-active">Блог компании Neoflex</a></li><li class="tm-separated-list__item"><a href="/ru/hub/machine_learning/" class="tm-hubs-list__link">Машинное обучение</a></li></ul></div></div></article></div> <!----></div> <div class="tm-article-sticky-panel"><div class="tm-data-icons tm-article-sticky-panel__icons"><div class="tm-article-rating tm-data-icons__item"><div class="tm-votes-meter tm-article-rating__votes-switcher"><svg height="24" width="24" class="tm-svg-img tm-votes-meter__icon tm-votes-meter__icon tm-votes-meter__icon_appearance-article"><title>Всего голосов 13: ↑13 и ↓0</title> <use xlink:href="/img/megazord-v28.617e16ca..svg#counter-rating"></use></svg> <span title="Всего голосов 13: ↑13 и ↓0" class="tm-votes-meter__value tm-votes-meter__value tm-votes-meter__value_positive tm-votes-meter__value_appearance-article tm-votes-meter__value_rating">+13</span></div> <DIV class="v-portal" style="display:none;"></DIV></div> <!----> <!----> <button title="Добавить в закладки" type="button" class="bookmarks-button tm-data-icons__item"><span class="tm-svg-icon__wrapper bookmarks-button__icon"><svg height="24" width="24" class="tm-svg-img tm-svg-icon"><title>Добавить в закладки</title> <use xlink:href="/img/megazord-v28.617e16ca..svg#counter-favorite"></use></svg></span> <span title="Количество пользователей, добавивших публикацию в закладки" class="bookmarks-button__counter">
    42
  </span></button> <div title="Поделиться" class="tm-sharing tm-data-icons__item"><button type="button" class="tm-sharing__button"><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" class="tm-sharing__icon"><path fill="currentColor" d="M13.8 13.8V18l7.2-6.6L13.8 5v3.9C5 8.9 3 18.6 3 18.6c2.5-4.4 6-4.8 10.8-4.8z"></path></svg></button> <DIV class="v-portal" style="display:none;"></DIV></div> <div title="Читать комментарии" class="tm-article-comments-counter-link tm-data-icons__item"><a href="/ru/company/neoflex/blog/722584/comments/" class="tm-article-comments-counter-link__link"><svg height="24" width="24" class="tm-svg-img tm-article-comments-counter-link__icon"><title>Комментарии</title> <use xlink:href="/img/megazord-v28.617e16ca..svg#counter-comments"></use></svg> <span class="tm-article-comments-counter-link__value">
      2
    </span></a> <!----></div> <!----> <DIV class="v-portal" style="display:none;"></DIV></div></div></div>  <div class="tm-article-presenter__footer"><div class="tm-article-blocks"><!----> <div></div> <section class="tm-block tm-block tm-block_spacing-bottom"><!----> <div class="tm-block__body tm-block__body tm-block__body_variant-balanced"><div class="tm-article-author"><div class="tm-article-author__company"><div class="tm-article-author__company-card"><div class="tm-company-snippet"><a href="/ru/company/neoflex/profile/" class="tm-company-snippet__logo-link"><div class="tm-entity-image"><img alt="" height="40" src="//habrastorage.org/getpro/habr/company/815/dfd/395/815dfd395f12e5979bea705bdbea89ea.png" width="40" class="tm-entity-image__pic"></div></a> <div class="tm-company-snippet__info"><a href="/ru/company/neoflex/profile/" class="tm-company-snippet__title">Neoflex</a> <div class="tm-company-snippet__description">Создаем ИТ-платформы для цифровой трансформации</div></div></div> <div class="tm-article-author__buttons"><!----> <!----></div></div> <div class="tm-article-author__company-contacts"><a href="http://www.neoflex.ru/" rel="noopener" target="_blank" class="tm-article-author__contact">
      Сайт
    </a><a href="https://facebook.com/neoflexcompany" rel="noopener" target="_blank" class="tm-article-author__contact">
      Facebook
    </a><a href="https://telegram.me/neoflexlive" rel="noopener" target="_blank" class="tm-article-author__contact">
      Telegram
    </a></div> <div class="tm-article-author__separator"></div></div> <div class="tm-user-card tm-article-author__user-card tm-user-card tm-user-card_variant-article"><div class="tm-user-card__info-container"><div class="tm-user-card__header"><div class="tm-user-card__header-data"><a href="/ru/users/neoflex/" class="tm-user-card__userpic tm-user-card__userpic_size-40"><div class="tm-entity-image"><img alt="" src="//habrastorage.org/getpro/habr/avatars/9f6/8f0/bda/9f68f0bda31b08e17c0d6a2397a2e4a1.png" class="tm-entity-image__pic"></div></a> <div class="tm-user-card__meta"><div title=" 39 голосов " class="tm-counter-container tm-karma tm-karma"><div class="tm-counter-container__header"><div class="tm-karma__votes tm-karma__votes_positive">
      23
    </div></div> <div class="tm-counter-container__footer"><div class="tm-karma__text">
      Карма
    </div> <DIV class="v-portal" style="display:none;"></DIV></div></div> <div title="Рейтинг пользователя" class="tm-counter-container"><div class="tm-counter-container__header"> <div class="tm-votes-lever tm-votes-lever tm-votes-lever_appearance-rating"><!----> <div class="tm-votes-lever__score tm-votes-lever__score tm-votes-lever__score_appearance-rating"><span class="tm-votes-lever__score-counter tm-votes-lever__score-counter tm-votes-lever__score-counter_rating">
        25
      </span></div> <!----></div></div> <div class="tm-counter-container__footer"><span class="tm-rating__text tm-rating__text">
      Рейтинг
    </span></div></div></div></div></div> <div class="tm-user-card__info tm-user-card__info tm-user-card__info_variant-article"><div class="tm-user-card__title tm-user-card__title tm-user-card__title_variant-article"><!----> <a href="/ru/users/neoflex/" class="tm-user-card__nickname tm-user-card__nickname tm-user-card__nickname_variant-article">
          @neoflex
        </a> <!----></div> <p class="tm-user-card__short-info tm-user-card__short-info tm-user-card__short-info_variant-article">Пользователь</p></div></div> <div class="tm-user-card__buttons tm-user-card__buttons tm-user-card__buttons_variant-article"><!----> <!----> <!----> <!----> <!----></div></div> <!----></div> <DIV class="v-portal" style="display:none;"></DIV></div> <!----></section> <!----> <div class="tm-article-blocks__comments"><div class="tm-article-page-comments"><div class="tm-article-comments-counter-link tm-article-comments-counter-button"><a href="/ru/company/neoflex/blog/722584/comments/" class="tm-article-comments-counter-link__link tm-article-comments-counter-link__link_button-style"><svg height="24" width="24" class="tm-svg-img tm-article-comments-counter-link__icon tm-article-comments-counter-link__icon_contrasted"><title>Комментарии</title> <use xlink:href="/img/megazord-v28.617e16ca..svg#counter-comments"></use></svg> <span class="tm-article-comments-counter-link__value tm-article-comments-counter-link__value_contrasted">
       Комментарии 2 
    </span></a> <!----></div></div></div>  <section class="tm-block tm-block tm-block_spacing-bottom"><header class="tm-block__header tm-block__header tm-block__header_variant-borderless"><div class="tm-block__header-container"><h2 class="tm-block__title tm-block__title tm-block__title_variant-large">Публикации</h2> </div> <!----></header> <div class="tm-block__body tm-block__body tm-block__body_variant-condensed-slim"><div class="tm-tabs tm-tabs"><div><span class="tm-tabs__tab-item"><button class="tm-tabs__tab-link tm-tabs__tab-link tm-tabs__tab-link_active tm-tabs__tab-link_slim">
        Лучшие за сутки
      </button></span><span class="tm-tabs__tab-item"><button class="tm-tabs__tab-link tm-tabs__tab-link tm-tabs__tab-link_slim">
        Похожие
      </button></span></div> <!----></div> <div class="similar-and-daily__tab-view"><div><!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <div class="tm-placeholder-article-cards"><div class="tm-placeholder-article-card"><div class="tm-placeholder__user"><div class="tm-placeholder__user-pic loads"></div> <div class="tm-placeholder__user-date loads"></div></div> <div class="tm-placeholder-article-card__title"><div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div> <div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div></div> <div class="tm-placeholder-article-card__icons tm-placeholder__counters"><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div></div></div><div class="tm-placeholder-article-card"><div class="tm-placeholder__user"><div class="tm-placeholder__user-pic loads"></div> <div class="tm-placeholder__user-date loads"></div></div> <div class="tm-placeholder-article-card__title"><div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div> <div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div></div> <div class="tm-placeholder-article-card__icons tm-placeholder__counters"><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div></div></div><div class="tm-placeholder-article-card"><div class="tm-placeholder__user"><div class="tm-placeholder__user-pic loads"></div> <div class="tm-placeholder__user-date loads"></div></div> <div class="tm-placeholder-article-card__title"><div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div> <div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div></div> <div class="tm-placeholder-article-card__icons tm-placeholder__counters"><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div></div></div><div class="tm-placeholder-article-card"><div class="tm-placeholder__user"><div class="tm-placeholder__user-pic loads"></div> <div class="tm-placeholder__user-date loads"></div></div> <div class="tm-placeholder-article-card__title"><div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div> <div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div></div> <div class="tm-placeholder-article-card__icons tm-placeholder__counters"><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div></div></div><div class="tm-placeholder-article-card"><div class="tm-placeholder__user"><div class="tm-placeholder__user-pic loads"></div> <div class="tm-placeholder__user-date loads"></div></div> <div class="tm-placeholder-article-card__title"><div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div> <div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div></div> <div class="tm-placeholder-article-card__icons tm-placeholder__counters"><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div></div></div></div> <!----> <!----> <!----> <!----></div> <!----></div></div> <!----></section> <div><!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <div class="tm-placeholder-inset tm-placeholder-vacancies"><div class="tm-placeholder-inset__header"><div class="tm-placeholder__line tm-placeholder__line_inset-header loads"></div></div> <div class="tm-placeholder-inset__body"><ul class="tm-placeholder-list"><li class="tm-placeholder-list__item tm-placeholder-list__item_inset"><div class="tm-placeholder-list__title-container"><div class="tm-placeholder__line tm-placeholder__line_item-title loads"></div></div> <div class="tm-project-block-items__properties"><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span></div></li><li class="tm-placeholder-list__item tm-placeholder-list__item_inset"><div class="tm-placeholder-list__title-container"><div class="tm-placeholder__line tm-placeholder__line_item-title loads"></div></div> <div class="tm-project-block-items__properties"><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span></div></li><li class="tm-placeholder-list__item tm-placeholder-list__item_inset"><div class="tm-placeholder-list__title-container"><div class="tm-placeholder__line tm-placeholder__line_item-title loads"></div></div> <div class="tm-project-block-items__properties"><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span></div></li><li class="tm-placeholder-list__item tm-placeholder-list__item_inset"><div class="tm-placeholder-list__title-container"><div class="tm-placeholder__line tm-placeholder__line_item-title loads"></div></div> <div class="tm-project-block-items__properties"><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span></div></li><li class="tm-placeholder-list__item tm-placeholder-list__item_inset"><div class="tm-placeholder-list__title-container"><div class="tm-placeholder__line tm-placeholder__line_item-title loads"></div></div> <div class="tm-project-block-items__properties"><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span></div></li></ul></div> <div class="tm-placeholder-inset__footer"><div class="tm-placeholder__line tm-placeholder__line_inset-footer loads"></div></div></div> <!----> <!----> <!----> <!----> <!----> <!----></div> <!----> </div></div></div></div></div> <div class="tm-page__sidebar"><!----></div></div></div></div></main> <!----></div> <!----> <div class="tm-footer"><div class="tm-page-width"><div class="tm-footer__container"><div class="tm-footer__title"><a href="/ru/" class="tm-svg-icon__wrapper tm-footer__title-link router-link-active"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Хабр</title> <use xlink:href="/img/habr-logo-ru.svg#logo"></use></svg></a></div> <div class="tm-footer__social"><a href="https://www.facebook.com/habrahabr.ru" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Facebook</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-facebook"></use></svg></a><a href="https://twitter.com/habr_com" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Twitter</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-twitter"></use></svg></a><a href="https://vk.com/habr" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>VK</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-vkontakte"></use></svg></a><a href="https://telegram.me/habr_com" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Telegram</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-telegram"></use></svg></a><a href="https://www.youtube.com/channel/UCd_sTwKqVrweTt4oAKY5y4w" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Youtube</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-youtube"></use></svg></a><a href="https://zen.yandex.ru/habr" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Яндекс Дзен</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-zen"></use></svg></a></div> <DIV class="v-portal" style="display:none;"></DIV> <button class="tm-footer__link"><svg height="16" width="16" class="tm-svg-img tm-footer__icon"><title>Язык</title> <use xlink:href="/img/megazord-v28.617e16ca..svg#lang"></use></svg>
        Настройка языка
      </button> <a href="/ru/feedback/" class="tm-footer__link">
        Техническая поддержка
      </a> <a href="/berserk-mode-nope" class="tm-footer__link">
        Вернуться на старую версию
      </a> <div class="tm-footer-copyright"><span class="tm-copyright"><span class="tm-copyright__years">© 2006–2023, </span> <span class="tm-copyright__name"><a href="https://company.habr.com/" rel="noopener" target="_blank" class="tm-copyright__link">Habr</a></span></span></div></div></div></div> <!----> <!----></div> <div class="vue-portal-target"></div></div>
<script>window.__INITIAL_STATE__={"adblock":{"hasAcceptableAdsFilter":false,"hasAdblock":false},"articlesList":{"articlesList":{"722584":{"id":"722584","timePublished":"2023-03-15T13:10:28+00:00","isCorporative":true,"lang":"ru","titleHtml":"Как воспитать GPT-3 модель в домашних условиях","leadData":{"textHtml":"\u003Cp\u003EМы решили проверить технологию, на которой основан ChatGPT, посмотреть актуальное состояние open-source GPT-3-like моделей и ответить на вопрос — можно ли обучить GPT-3-like модель в домашних условиях?\u003C\u002Fp\u003E\u003Cp\u003EДля эксперимента выбрали GPT-J и не самый мощный ПК с видеокартой Nvidia GTX 1080TI с 11 GB VRAM. Оказалось, что этого достаточно не только, чтобы загрузить модель, но и дообучить&nbsp;ее (fine-tune). Рассказываем — как мы это сделали.\u003C\u002Fp\u003E\u003Cp\u003E\u003C\u002Fp\u003E","imageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fc90\u002Fe79\u002F121\u002Fc90e79121c6be5565f88ee6142bfbb1e.png","buttonTextHtml":"Читать далее","image":{"url":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fc90\u002Fe79\u002F121\u002Fc90e79121c6be5565f88ee6142bfbb1e.png","fit":"cover","positionY":0,"positionX":0}},"editorVersion":"2.0","postType":"article","postLabels":[],"author":{"id":"1688581","alias":"neoflex","fullname":null,"avatarUrl":"\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Favatars\u002F9f6\u002F8f0\u002Fbda\u002F9f68f0bda31b08e17c0d6a2397a2e4a1.png","speciality":"Пользователь","scoreStats":{"score":23,"votesCount":39},"rating":25,"relatedData":null,"contacts":[],"authorContacts":[],"paymentDetails":{"paymentYandexMoney":null,"paymentPayPalMe":null,"paymentWebmoney":null}},"statistics":{"commentsCount":2,"favoritesCount":42,"readingCount":2061,"score":13,"votesCount":13,"votesCountPlus":13,"votesCountMinus":0},"hubs":[{"id":"21430","alias":"neoflex","type":"corporative","title":"Блог компании Neoflex","titleHtml":"Блог компании Neoflex","isProfiled":false,"relatedData":null},{"id":"19439","alias":"machine_learning","type":"collective","title":"Машинное обучение","titleHtml":"Машинное обучение","isProfiled":true,"relatedData":null}],"flows":[{"id":"1","alias":"develop","title":"Разработка","titleHtml":"Разработка"}],"relatedData":null,"textHtml":"\u003Cdiv xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxhtml\"\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fc90\u002Fe79\u002F121\u002Fc90e79121c6be5565f88ee6142bfbb1e.png\" width=\"780\" height=\"440\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fc90\u002Fe79\u002F121\u002Fc90e79121c6be5565f88ee6142bfbb1e.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EЧат-бот ChatGPT наделал много шума: люди задают ему вопросы и удивляются точности ответов, студенты с его помощью пишут дипломные работы, а копирайтеры публикуют статьи с названием типа «Сможет ли ChatGPT заменить копирайтеров?». \u003C\u002Fp\u003E\u003Cp\u003EМы решили проверить технологию, на которой основан ChatGPT, посмотреть актуальное состояние open-source GPT-3-like моделей и ответить на вопрос — можно ли обучить GPT-3-like модель в домашних условиях?\u003C\u002Fp\u003E\u003Cp\u003EДля эксперимента выбрали GPT-J и не самый мощный ПК с видеокартой Nvidia GTX 1080TI с 11 GB VRAM. Оказалось, что этого достаточно не только, чтобы загрузить модель, но и дообучить ее (fine-tune). Рассказываем — как мы это сделали.\u003C\u002Fp\u003E\u003Ch3\u003EПочему именно GPT-J \u003C\u002Fh3\u003E\u003Cp\u003EGPT (англ. Generative Pre-trained Transformer — генеративный предобученный трансформер) — это гигантская нейронная сеть, которая обучена на огромном количестве данных предсказывать следующее за последовательностью слово. GPT моделирует естественные языки, помнит контекст и генерирует тексты. Для тех, кто хочет узнать побольше про архитектуру GPT, мы оставим ссылки [\u003Cspan\u003E\u003Cu\u003E1\u003C\u002Fu\u003E\u003C\u002Fspan\u003E] и [\u003Cspan\u003E\u003Cu\u003E15\u003C\u002Fu\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Cp\u003EПонять преимущества модели GPT-J от команды EleutherAI проще всего, если взглянуть на таблицу, которую привели сами разработчики. Мы покажем ее в сокращенном виде. Полную версию можно посмотреть по ссылке [\u003Cspan\u003E\u003Cu\u003E2\u003C\u002Fu\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F3ac\u002F3eb\u002Fd07\u002F3ac3ebd0715af23417ed59494ead9d61.png\" alt=\"Сравнение GPT-like моделей. \" title=\"Сравнение GPT-like моделей. \" width=\"846\" height=\"525\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F3ac\u002F3eb\u002Fd07\u002F3ac3ebd0715af23417ed59494ead9d61.png\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003EСравнение GPT-like моделей. \u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EНасколько хороша модель, можно увидеть по значениям бенчмарков:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003E\u003Cstrong\u003ELAMBADA \u003C\u002Fstrong\u003E— это датасет, который используется как бенчмарк для предсказания слова по широкому контексту. Для этого бенчмарка в таблице приведены две метрики: перплексия \u003Cstrong\u003EPPl \u003C\u002Fstrong\u003E(меньше — лучше) и точность \u003Cstrong\u003EAcc \u003C\u002Fstrong\u003E(выше — лучше);\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Cstrong\u003EWinorgande \u003C\u002Fstrong\u003E— это датасет с текстовыми задачами и вариантами ответов;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Cstrong\u003EHellaswag \u003C\u002Fstrong\u003E— датасет с текстовыми задачами на здравый смысл;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Cstrong\u003EPIQA \u003C\u002Fstrong\u003E— также бенчмарк для оценки здравого смысла – нацелен на оценку физических знаний модели.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003EПо показателям видно, что среди public-моделей GPT-J выглядит лучше всех. А в сравнении с моделями от OpenAI GPT-J показывает сопоставимые результаты с моделью Curie и уступает лишь гигантской Davinci. Объем тренировочного датасета GPT-J также внушителен — 825 GB [\u003Cspan\u003E\u003Cu\u003E4\u003C\u002Fu\u003E\u003C\u002Fspan\u003E], а количество параметров (весов) модели ~ 6 миллиардов. \u003C\u002Fp\u003E\u003Cp\u003EВ качестве основного стека для дообучения GPT-J мы выбрали Python, transformers и pytorch lightning. GPT-J и другие достижения open source-сообщества можно найти на Hugging Face Hub [\u003Cspan\u003E\u003Cu\u003E5\u003C\u002Fu\u003E\u003C\u002Fspan\u003E]. \u003C\u002Fp\u003E\u003Ch3\u003E8 bit GPT-J\u003C\u002Fh3\u003E\u003Cp\u003EМодель от EleutherAI с float16 параметрами требует около 21 GB VRAM — а это для нас проблема. Чтобы как-то уместить этого монстра на наш ПК, мы можем прибегнуть к квантизации [\u003Cspan\u003E\u003Cu\u003E6\u003C\u002Fu\u003E\u003C\u002Fspan\u003E, \u003Cspan\u003E\u003Cu\u003E7\u003C\u002Fu\u003E\u003C\u002Fspan\u003E] — хитрым способом сопоставить оригинальные float16 параметры модели с int8 параметрами и уменьшить объем занимаемой памяти в два раза. Кажется, что будет потеря в точности, но судя по результатам сравнения GPT-J и GPT-J-8bit, разница в точности этих моделей в пределах погрешности [\u003Cspan\u003E\u003Cu\u003E8\u003C\u002Fu\u003E\u003C\u002Fspan\u003E].\u003C\u002Fp\u003E\u003Cp\u003EКвантизованная модель GPT-J [\u003Cspan\u003E\u003Cu\u003E3\u003C\u002Fu\u003E\u003C\u002Fspan\u003E] доступна благодаря ребятам из \u003Cspan\u003E\u003Cu\u003ETraining Transformers Together\u003C\u002Fu\u003E\u003C\u002Fspan\u003E [\u003Cspan\u003E\u003Cu\u003E9\u003C\u002Fu\u003E\u003C\u002Fspan\u003E] и занимает примерно 6 GB на GTX 1080 TI. Давайте с ней поздороваемся.\u003C\u002Fp\u003E\u003Cfigure class=\"bordered full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fc5a\u002F434\u002F2bf\u002Fc5a4342bf3b48d6b076f810f40ee76d4.png\" width=\"597\" height=\"289\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fc5a\u002F434\u002F2bf\u002Fc5a4342bf3b48d6b076f810f40ee76d4.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EПервым делом нужно разобраться, что принимает и возвращает модель — input и output соответственно, и какая у этого размерность. Модель принимает последовательность слов и возвращает вероятности следующего слова для каждого из последовательности. GPT очень внимательный (привет, attention!) слушатель, который, услышав слово, сразу же пытается предсказать следующее.  \u003C\u002Fp\u003E\u003Ch3\u003EInput\u003C\u002Fh3\u003E\u003Cp\u003EТак выглядят входные данные:\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fcd9\u002F65f\u002F122\u002Fcd965f12248197356e6c911ff2a5d028.png\" width=\"890\" height=\"229\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fcd9\u002F65f\u002F122\u002Fcd965f12248197356e6c911ff2a5d028.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EМодель принимает вектор токенов (\u003Cstrong\u003Einput_ids\u003C\u002Fstrong\u003E) и \u003Cstrong\u003Eattention mask\u003C\u002Fstrong\u003E. Оба элемента с максимальной длиной 2048. \u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003E\u003Cstrong\u003EВектор токенов\u003C\u002Fstrong\u003E состоит из целых чисел: id токенов из словаря модели. Словарь модели содержит 50257 токенов; \u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Cstrong\u003EAttention mask\u003C\u002Fstrong\u003E: вектор с нулями и единицами, задача которого — сообщить модели – на какие токены обращать внимание (1), а на какие — нет (0).\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003EПроще говоря, нам нужно общаться с GPT словами, которые уже есть в его словаре, а также акцентировать внимание на важных для беседы словах. Например, есть несколько случаев, когда мы используем 0 в attention mask. Во-первых, мы используем 0 для паддинг-токенов, когда собираем сэмплы в батч (порцию для обучения) и нам нужно сделать паддинг, чтобы длины сэмплов совпадали. Во-вторых, в процессе обучения мы можем заставить модель не обращать внимания на некоторые токены в сэмпле (например, на те, которые мы хотим предсказать). \u003C\u002Fp\u003E\u003Cp\u003EНа иллюстрации выше видно, как sample_text превращается в input_ids и attention_mask.\u003C\u002Fp\u003E\u003Ch3\u003EOutput \u003C\u002Fh3\u003E\u003Cp\u003EВнимательный GPT постоянно пытается предсказать следующее слово, но кроме внимательности мы еще можем рассчитывать на его хорошую память. Модель по умолчанию возвращает вероятности следующих токенов (\u003Cstrong\u003Elogits\u003C\u002Fstrong\u003E) и внутреннюю память модели (\u003Cstrong\u003Epast_key_values\u003C\u002Fstrong\u003E). Так выглядят output нашей модели:\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F21f\u002F99e\u002F019\u002F21f99e019601451d43a5b4a8207907fe.png\" width=\"859\" height=\"558\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F21f\u002F99e\u002F019\u002F21f99e019601451d43a5b4a8207907fe.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EРазмер тензора \u003Cstrong\u003Elogits \u003C\u002Fstrong\u003E— (batch_size, sample_len, vocab_size). Наш исходный сэмпл 'Hello, GPT-J! How are you?' содержит 12 токенов, поэтому logits имеет форму (1, 12, 50400). \u003C\u002Fp\u003E\u003Cp\u003EДля каждого токена в исходном сэмпле мы получаем вероятности для всех токенов в словаре модели. При генерации текста нас интересует только вероятность для последнего токена. В дообучении модели могут участвовать все или только избранные вероятности для входной последовательности. \u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003E\u003Cstrong\u003Epast_key_value\u003C\u002Fstrong\u003E — это и есть память нашей модели. Размер тензора: (n_layers, key_value, batch, n_attention_heads, sample_len, head_embedding_dimension);\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Cstrong\u003En_layers \u003C\u002Fstrong\u003E— это количество слоев GPT-J;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Cstrong\u003Ekey_value\u003C\u002Fstrong\u003E — кортеж из ключей и значений в контексте механизма внимания (Attention) [\u003Cspan\u003E\u003Cu\u003E10\u003C\u002Fu\u003E\u003C\u002Fspan\u003E];\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Cstrong\u003Ebatch\u003C\u002Fstrong\u003E — размер батча; \u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Cstrong\u003En_attention_heads\u003C\u002Fstrong\u003E — количество голов attention;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Cstrong\u003Esample_len \u003C\u002Fstrong\u003E— длина сэмпла;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Cstrong\u003Ehead_embedding_dimension\u003C\u002Fstrong\u003E — внутренний размер attention head;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Cstrong\u003En_layers, key_value, n_attention_heads, head_embedding_dimension\u003C\u002Fstrong\u003E — размерности, которые относятся к конфигурации модели.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch3\u003EТестовая задача\u003C\u002Fh3\u003E\u003Cp\u003EПредставим, что мы хотим научить GPT модерировать чаты — модель будет классифицировать сообщения на 3 класса: hate (содержащее ненависть), offensive (содержащее угрозу), neutral (нейтральное). \u003C\u002Fp\u003E\u003Cp\u003EДля этого будем использовать Hate Speech and Offensive Language Dataset [\u003Cspan\u003E\u003Cu\u003E11\u003C\u002Fu\u003E\u003C\u002Fspan\u003E]. Поскольку GPT уже много знает о языке и словах, мы уменьшим количество тренировочных данных и возьмем случайным образом лишь 1000 примеров, предварительно сбалансировав классы. Для валидации выберем 200 сбалансированных примеров. Нам предстоит обучить GPT-J-8bit генерировать нужную метку класса.\u003C\u002Fp\u003E\u003Ch4\u003EКак подготовить данные?\u003C\u002Fh4\u003E\u003Cp\u003EКогда мы хотим чему-то научить GPT, нам нужно сказать ему начало фразы, а затем оценить его вариант завершения фразы, поэтому тренировочный сэмпл содержит две части — затравку и завершение.\u003C\u002Fp\u003E\u003Cp\u003EПодготовка входного сэмпла играет важную роль для обучения модели. В тренировочный сэмпл, кроме самого текста, который подлежит классификации, можно добавить дополнительную информацию: начиная от специальных разделителей блоков сэмпла и заканчивая целыми инструкциями. К важному можно отнести специальные токены, которые отделяют затравку от завершения (то есть от того, чему мы хотим обучить модель). С помощью токенов-разделителей мы как бы указываем –  когда ждем от нашего GPT выполнения желаемого.  \u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Ff8a\u002Fe01\u002F0d7\u002Ff8ae010d70d27c8754d42562f0a5a112.png\" width=\"874\" height=\"209\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Ff8a\u002Fe01\u002F0d7\u002Ff8ae010d70d27c8754d42562f0a5a112.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EНам удалось добиться одинаково хорошей точности как в случае с затравками-инструкциями, так и в случае затравок, состоящих только из целевого текста с разделителем. \u003C\u002Fp\u003E\u003Cp\u003EЕдинственный минус инструкций состоит в том, что входные сэмплы получаются длиннее. Но зато, благодаря инструкциям, модель лучше научится понимать – чего мы от нее хотим: GPT видит инструкцию и, соотнося ее с результатом, понимает, что все варианты ответов перечислены в инструкции и ей нужно только выбрать правильный. Это дает возможность научить GPT решать другие классы задач, которые могут быть сформулированы в рамках той же структуры инструкций. \u003C\u002Fp\u003E\u003Ch4\u003EЧто обучать? (Low-Rank Adapters)\u003C\u002Fh4\u003E\u003Cp\u003EВ GPT-J-8bit параметры квантизованы. Тренировка квантизованных целочисленных параметров привычными алгоритмами не представляется разумным подходом хотя бы потому, что область значений функции потерь cross entropy loss лежит в [0, 1]. Но даже квантизация не избавляет нас от тренировки огромного количества параметров и затрат на вычисления.\u003C\u002Fp\u003E\u003Cp\u003EАвторы статьи \u003Cem\u003ELoRA: Low-Rank Adaption of Large Language Models\u003C\u002Fem\u003E [\u003Cspan\u003E\u003Cu\u003E12\u003C\u002Fu\u003E\u003C\u002Fspan\u003E] предлагают очень интересный и эффективный способ дообучения огромных моделей. Зачем тренировать все параметры модели? Давайте заморозим все слои и к каждому добавим тренируемый адаптер с низкой размерностью. Адаптер представляет собой два линейных слоя A и B размера (d, r) и (r, d). И вместо тренировки «родного» слоя модели W размера (d,d) (а у  GPT-J d равен 4096) предлагается тренировать две матрицы поменьше. Авторы LoRA демонстрируют, что оптимальный r равен 2, чем оправдывают название своего подхода Low-Rank Adaption. Его логика отображена на схеме:\u003C\u002Fp\u003E\u003Cfigure class=\"bordered \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fbd6\u002F82f\u002Fdf8\u002Fbd682fdf853c1546488055f9afe606c6.png\" alt=\"LoRA\" title=\"LoRA\" width=\"292\" height=\"255\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fbd6\u002F82f\u002Fdf8\u002Fbd682fdf853c1546488055f9afe606c6.png\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003ELoRA\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Ch4\u003EКак обучать? (Adam8bit)\u003C\u002Fh4\u003E\u003Cp\u003EЗагрузить модель в оперативную память видеокарты — это еще не приручить ее. Для дообучения GPT стандартными техниками требуется еще одна видеокарта, но мы обойдемся без нее и воспользуемся квантизованным оптимизатором. \u003C\u002Fp\u003E\u003Cp\u003EСтандартным инструментом обучения параметров модели является оптимизатор Adam. Нам же нужен какой-то экономный аналог. В статье \u003Cem\u003E8-Bit Optimizers via Block-wise Quantization\u003C\u002Fem\u003E [\u003Cspan\u003E\u003Cu\u003E13\u003C\u002Fu\u003E\u003C\u002Fspan\u003E] авторы предлагают квантизовать оптимизатор, в частности, его состояния, которые включают статистику по градиентам (поправкам) для параметров модели. Более того, предлагается блочная (block-wise) квантизация, которая может выполняться параллельно. Картинка из оригинальной статьи прекрасно иллюстрирует суть этого:\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fdd4\u002F77c\u002F578\u002Fdd477c57808073c4fc001dc9f4fd8101.png\" width=\"667\" height=\"319\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fdd4\u002F77c\u002F578\u002Fdd477c57808073c4fc001dc9f4fd8101.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EСпасибо авторам за то, что они также опубликовали репозиторий bitsandbytes [\u003Cspan\u003E\u003Cu\u003E14\u003C\u002Fu\u003E\u003C\u002Fspan\u003E] с реализацией их подхода. \u003C\u002Fp\u003E\u003Ch4\u003EПоследние приготовления и обучение\u003C\u002Fh4\u003E\u003Cp\u003EМы описали ключевые способы, которые помогут нам приручить GPT-J. Но для того, чтобы научить GPT делать то, что нам нужно, необходимо как-то указывать ему на его ошибки. Мы используем \u003Cstrong\u003Eloss mask\u003C\u002Fstrong\u003E в своем коде для того, чтобы взять из предсказаний модели только то, что должно быть сгенерировано, и сделать обратное распространение ошибки только по этим токенам. Так мы не будем считать ошибку на затравке, а посчитаем loss только на продолжении. И модель будет лучше понимать, что мы от нее хотим. Также если мы решаем задачу классификации, можно подсчитать точность предсказаний — это и будет метрикой оценки модели.\u003C\u002Fp\u003E\u003Cp\u003EВот так выглядит шаг обучения модели:\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fdab\u002Fb03\u002F6df\u002Fdabb036df12a0f8f9d521b2f66bb947e.png\" width=\"560\" height=\"392\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fdab\u002Fb03\u002F6df\u002Fdabb036df12a0f8f9d521b2f66bb947e.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EВ зависимости от длины сэмплов и объема памяти видеокарты мы можем использовать батчи. Нам потребуется привести к одинаковой длине все сэмплы — сделать паддинг коротких сэмплов и обрезать длинные. Батчинг поможет ускорить процесс обучения. \u003C\u002Fp\u003E\u003Cp\u003EПосле того, как архитектура GPT-J с адаптерами собрана, данные для обучения загружены и предобработаны, а оптимизатор Adam8bit наготове. Мы можем приступать к fine-tuning’у и валидации. \u003C\u002Fp\u003E\u003Cp\u003EБудем обучать модель на протяжении трёх эпох. Также сравним четыре подхода:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003EОбучение 1D параметров модели (те слои, где использование адаптера бессмысленно) — 861 K обучаемых параметров;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EАдаптеры только для attention слоев — 2.7 M обучаемых параметров;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EАдаптеры для всех слоев — 5.2 М обучаемых параметров;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EFew-shot — без обучения.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003EМы имеем дело с «умной» моделью, которая много знает о языке, поэтому можем вообще отказаться от тренировочных данных и положиться на саму модель. Few-shot подход, описанный в статье от команды OpenAI [\u003Cspan\u003E\u003Cu\u003E15\u003C\u002Fu\u003E\u003C\u002Fspan\u003E], предлагает использовать несколько тренировочных сэмплов в затравке, получая в результате завершения для новых случаев. \u003C\u002Fp\u003E\u003Ch4\u003EРезультаты \u003C\u002Fh4\u003E\u003Cp\u003EДля начала взглянем на отчет [\u003Cspan\u003E\u003Cu\u003E16\u003C\u002Fu\u003E\u003C\u002Fspan\u003E] по обучению для нашей GPT-J. График с \u003Cstrong\u003Eloss \u003C\u002Fstrong\u003Eнам нужен, чтобы убедиться, что модель обучается и loss уменьшается со временем от эпохи к эпохе. А от \u003Cstrong\u003Eaccuracy \u003C\u002Fstrong\u003Eмы ожидаем, что она будет расти.\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F8a3\u002F8c3\u002F3d6\u002F8a38c33d6362b78738016fe49b59fbeb.png\" alt=\"GPT-J fine-tuning report \" title=\"GPT-J fine-tuning report \" width=\"764\" height=\"602\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F8a3\u002F8c3\u002F3d6\u002F8a38c33d6362b78738016fe49b59fbeb.png\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003EGPT-J fine-tuning report \u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EТеперь испытаем модели OpenAI через API в идентичных условиях. Вот отчет по fine-tuning’у GPT-3 Ada и GPT-3 Davinci [\u003Cspan\u003E\u003Cu\u003E17\u003C\u002Fu\u003E\u003C\u002Fspan\u003E]. Предположительно, значение loss отличаются на порядок от нашей модели из-за различия функций потерь. В нашем подходе мы использовали стандартную cross entropy. \u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F296\u002F183\u002F1c5\u002F2961831c58cc711297ac1415e35cea76.png\" alt=\"OpenAI’s models fine-tuning report \" title=\"OpenAI’s models fine-tuning report \" width=\"720\" height=\"569\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F296\u002F183\u002F1c5\u002F2961831c58cc711297ac1415e35cea76.png\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003EOpenAI’s models fine-tuning report \u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EВзглянем на таблицу точности предсказаний всех опробованных моделей и подходов:\u003C\u002Fp\u003E\u003Cfigure class=\"\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fc44\u002F0e0\u002F313\u002Fc440e0313411d4b96a92bace0feebf55.png\" alt=\"Сравнение GPT-J и моделей OpenAI\" title=\"Сравнение GPT-J и моделей OpenAI\" width=\"514\" height=\"235\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fc44\u002F0e0\u002F313\u002Fc440e0313411d4b96a92bace0feebf55.png\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003EСравнение GPT-J и моделей OpenAI\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EНам удалось превзойти точность моделей от OpenAI в тестовой задаче модерации при идентичных условиях. Адаптеры предлагают наилучшую точность, и чем их больше, тем точнее предсказания модели. Few-shot подход оставляет желать лучшего. А ChatGPT (GPT-3.5-turbo) дает весьма посредственную точность, хотя и лучше, чем GPT-J без дообучения.\u003C\u002Fp\u003E\u003Cp\u003EМы смогли обучить  GPT-J в домашних условиях, причем так, что он не уступает разработкам больших компаний. Однако важно отметить, что если дообучать OpenAI GPT-3 модели предложенным в документации API способом — без инструкций, только сырой текст с разделителем — то Davinci демонстрирует идентичную точность — 84 %. С другой стороны, использование дообученных моделей OpenAI будет стоить денег: например, за генерацию приблизительно двух печатных страниц текста мы заплатим 12 центов для самой крутой модели, причем в стоимость входят и затравки.\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F4ea\u002Ff1f\u002F860\u002F4eaf1f86037ed3530ee32f2c02858914.png\" alt=\"Цена генерации дообученных моделей\" title=\"Цена генерации дообученных моделей\" width=\"611\" height=\"238\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F4ea\u002Ff1f\u002F860\u002F4eaf1f86037ed3530ee32f2c02858914.png\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003EЦена генерации дообученных моделей\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003EПреимущества инструкций в затравках\u003C\u002Fh3\u003E\u003Cp\u003EЧтобы подтвердить, что решение помещать в затравки инструкции действительно имеет преимущества, мы провели еще один эксперимент — предложили модели выбрать категорию для заголовков новостей с сайта BBC.\u003C\u002Fp\u003E\u003Cp\u003EДообученная модель показывает, что благодаря инструкциям в затравках она научилась понимать, что мы от нее хотим, без дополнительного обучения на новых категориях. \u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F411\u002Fad0\u002F838\u002F411ad08383e96d9500db5d33e27bd961.png\" alt=\"Классификация заголовков новостей\" title=\"Классификация заголовков новостей\" width=\"1203\" height=\"524\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F411\u002Fad0\u002F838\u002F411ad08383e96d9500db5d33e27bd961.png\"\u002F\u003E\u003Cdiv\u003E\u003Cfigcaption\u003EКлассификация заголовков новостей\u003C\u002Ffigcaption\u003E\u003C\u002Fdiv\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003EЗаключение\u003C\u002Fh3\u003E\u003Cp\u003EКогда мы заканчивали исследование, OpenAI предоставили доступ к API ChatGPT — gpt-3.5-turbo. Завышенные ожидания от технологии вызвали опасение, что gpt-3.5-turbo может обесценить это исследование. Но, как оказалось, чуда не произошло — результаты работы этой модели вы могли видеть в таблице выше. ChatGPT не может решить конкретную прикладную задачу так же хорошо, как дообученная модель.\u003C\u002Fp\u003E\u003Cp\u003EНам до сих пор нужно быть нянькой для искусственного интеллекта: готовить данные и придумывать все более эффективные способы дообучения. Одомашнить многообещающую открытую GPT помогли концепции тренируемых адаптеров и квантизуемого оптимизатора. Мы также предлагаем ознакомиться с репозиторием [\u003Cspan\u003E\u003Cu\u003E18\u003C\u002Fu\u003E\u003C\u002Fspan\u003E], который воспроизводит все полученные здесь результаты.  \u003C\u002Fp\u003E\u003Cp\u003E\u003Cstrong\u003EСсылки\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\u003Col\u003E\u003Cli\u003E\u003Cp\u003E\u003Cspan\u003E\u003Cu\u003EBuilding a GPT-like Model from Scratch with Detailed Theory and Code Implementation \u002F Хабр (habr.com)\u003C\u002Fu\u003E\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Cspan\u003E\u003Cu\u003EEleutherAI\u002Fgpt-j-6B · Hugging Face\u003C\u002Fu\u003E\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Cspan\u003E\u003Cu\u003Ehivemind\u002Fgpt-j-6B-8bit · Hugging Face\u003C\u002Fu\u003E\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Cspan\u003E\u003Cu\u003EThe Pile (eleuther.ai)\u003C\u002Fu\u003E\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Cspan\u003E\u003Cu\u003EModels - Hugging Face\u003C\u002Fu\u003E\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Cspan\u003E\u003Cu\u003EHow to accelerate and compress neural networks with quantization | by Tivadar Danka | Towards Data Science\u003C\u002Fu\u003E\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Cspan\u003E\u003Cu\u003EAchieving FP32 Accuracy for INT8 Inference Using Quantization Aware Training with NVIDIA TensorRT | NVIDIA Technical Blog\u003C\u002Fu\u003E\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Cspan\u003E\u003Cu\u003EJupyter Notebook Viewer (nbviewer.org)\u003C\u002Fu\u003E\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Ehttps:\u002F\u002Ftraining-transformers-together.github.io\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Cspan\u003E\u003Cu\u003E[1706.03762] Attention Is All You Need (arxiv.org)\u003C\u002Fu\u003E\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Cspan\u003E\u003Cu\u003EHate Speech and Offensive Language Dataset | Kaggle\u003C\u002Fu\u003E\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Cspan\u003E\u003Cu\u003E[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models (arxiv.org)\u003C\u002Fu\u003E\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Cspan\u003E\u003Cu\u003E[2110.02861] 8-bit Optimizers via Block-wise Quantization (arxiv.org)\u003C\u002Fu\u003E\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Cspan\u003E\u003Cu\u003ETimDettmers\u002Fbitsandbytes: 8-bit CUDA functions for PyTorch (github.com)\u003C\u002Fu\u003E\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Cspan\u003E\u003Cu\u003E[2005.14165] Language Models are Few-Shot Learners (arxiv.org)\u003C\u002Fu\u003E\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Cspan\u003E\u003Cu\u003Ehttps:\u002F\u002Fapi.wandb.ai\u002Flinks\u002Fvetka925\u002Fd7bb74kg\u003C\u002Fu\u003E\u003C\u002Fspan\u003E \u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Ehttps:\u002F\u002Fapi.wandb.ai\u002Flinks\u002Fvetka925\u002Fkrtz93ul\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Cspan\u003E\u003Cu\u003Evetka925\u002Fgpt-j-8bit-lightning-finetune (github.com)\u003C\u002Fu\u003E\u003C\u002Fspan\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cp\u003EАвтор статьи: Виталий Гречачин, Data Scientist в компании Neoflex.\u003C\u002Fp\u003E\u003Cp\u003E\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E","tags":[{"titleHtml":"gpt-3"},{"titleHtml":"chatgpt"},{"titleHtml":"quantization"},{"titleHtml":"lora"},{"titleHtml":"low-ranked adapters"},{"titleHtml":"fine-tuning"},{"titleHtml":"pytorch"},{"titleHtml":"gpt"},{"titleHtml":"gpt-j"},{"titleHtml":"text generation"}],"metadata":{"stylesUrls":[],"scriptUrls":[],"shareImageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fc90\u002Fe79\u002F121\u002Fc90e79121c6be5565f88ee6142bfbb1e.png","shareImageWidth":1200,"shareImageHeight":630,"vkShareImageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fc90\u002Fe79\u002F121\u002Fc90e79121c6be5565f88ee6142bfbb1e.png","schemaJsonLd":"{\"@context\":\"http:\\\u002F\\\u002Fschema.org\",\"@type\":\"Article\",\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https:\\\u002F\\\u002Fhabr.com\\\u002Fru\\\u002Fcompany\\\u002Fneoflex\\\u002Fblog\\\u002F722584\\\u002F\"},\"headline\":\"Как воспитать GPT-3 модель в домашних условиях\",\"datePublished\":\"2023-03-15T16:10:28+03:00\",\"dateModified\":\"2023-03-15T17:37:43+03:00\",\"author\":{\"@type\":\"Person\",\"name\":\"neoflex\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Habr\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fa_\\\u002Flk\\\u002F9m\\\u002Fa_lk9mjkccjox-zccjrpfolmkmq.png\"}},\"description\":\"Чат-бот ChatGPT наделал много шума: люди задают ему вопросы и удивляются точности ответов, студенты&nbsp;с его помощью пишут дипломные работы, а копирайтеры публикуют...\",\"url\":\"https:\\\u002F\\\u002Fhabr.com\\\u002Fru\\\u002Fcompany\\\u002Fneoflex\\\u002Fblog\\\u002F722584\\\u002F#post-content-body\",\"about\":[\"c_neoflex\",\"h_machine_learning\",\"f_develop\"],\"image\":[\"https:\\\u002F\\\u002Fhabr.com\\\u002Fshare\\\u002Fpublication\\\u002F722584\\\u002Ffb946f092cd29038a12ae4c19955245d\\\u002F\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002Fc90\\\u002Fe79\\\u002F121\\\u002Fc90e79121c6be5565f88ee6142bfbb1e.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F3ac\\\u002F3eb\\\u002Fd07\\\u002F3ac3ebd0715af23417ed59494ead9d61.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002Fc5a\\\u002F434\\\u002F2bf\\\u002Fc5a4342bf3b48d6b076f810f40ee76d4.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002Fcd9\\\u002F65f\\\u002F122\\\u002Fcd965f12248197356e6c911ff2a5d028.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F21f\\\u002F99e\\\u002F019\\\u002F21f99e019601451d43a5b4a8207907fe.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002Ff8a\\\u002Fe01\\\u002F0d7\\\u002Ff8ae010d70d27c8754d42562f0a5a112.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002Fbd6\\\u002F82f\\\u002Fdf8\\\u002Fbd682fdf853c1546488055f9afe606c6.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002Fdd4\\\u002F77c\\\u002F578\\\u002Fdd477c57808073c4fc001dc9f4fd8101.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002Fdab\\\u002Fb03\\\u002F6df\\\u002Fdabb036df12a0f8f9d521b2f66bb947e.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F8a3\\\u002F8c3\\\u002F3d6\\\u002F8a38c33d6362b78738016fe49b59fbeb.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F296\\\u002F183\\\u002F1c5\\\u002F2961831c58cc711297ac1415e35cea76.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002Fc44\\\u002F0e0\\\u002F313\\\u002Fc440e0313411d4b96a92bace0feebf55.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F4ea\\\u002Ff1f\\\u002F860\\\u002F4eaf1f86037ed3530ee32f2c02858914.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F411\\\u002Fad0\\\u002F838\\\u002F411ad08383e96d9500db5d33e27bd961.png\"]}","metaDescription":"Чат-бот ChatGPT наделал много шума: люди задают ему вопросы и удивляются точности ответов, студенты&nbsp;с его помощью пишут дипломные работы, а копирайтеры публикуют статьи с названием типа «Сможет...","mainImageUrl":null,"amp":true,"customTrackerLinks":[]},"polls":[],"commentsEnabled":true,"rulesRemindEnabled":false,"votesEnabled":true,"status":"published","plannedPublishTime":null,"checked":null,"hasPinnedComments":false,"format":"review","readingTime":10,"complexity":"medium","isEditorial":false}},"articlesIds":{},"isLoading":false,"pagesCount":{},"route":{},"reasonsList":null,"view":"list","lastVisitedRoute":{},"ssrCommentsArticleIds":[""],"karma":{"userReasonsList":null}},"authorContribution":{"authors":{}},"betaTest":{"currentAnnouncement":null,"announcements":{},"announcementCards":null,"announcementComments":{},"announcementCommentThreads":{},"announcementCommentingStatuses":{},"archivedList":[]},"authorStatistics":{"articleRefs":{},"articleIds":{},"pagesCount":{},"route":{},"viewsCount":[],"maxStatsCount":{}},"career":{"seoLandings":[],"hubs":""},"comments":{"articleComments":{},"articlePinnedComments":{},"searchCommentsResults":null,"pagesCount":null,"commentAccess":{},"scrollParents":{},"pageArticleComments":{"lastViewedComment":0,"postId":null,"lastCommentTimestamp":"","moderated":[],"moderatedIds":[],"commentRoute":""}},"companies":{"companyRefs":{"neoflex":{"alias":"neoflex","imageUrl":"\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fcompany\u002F815\u002Fdfd\u002F395\u002F815dfd395f12e5979bea705bdbea89ea.png","titleHtml":"Neoflex","descriptionHtml":"Создаем ИТ-платформы для цифровой трансформации","relatedData":null,"statistics":{"subscribersCount":20266,"rating":136.71,"invest":null,"postsCount":52,"newsCount":0,"vacanciesCount":49,"employeesCount":12,"careerRating":4.12},"foundationDate":{"year":"2005","month":"02","day":"21"},"location":{"city":{"id":"447159","title":"Москва"},"region":{"id":"1885","title":"Москва и Московская обл."},"country":{"id":"168","title":"Россия"}},"siteUrl":"http:\u002F\u002Fwww.neoflex.ru\u002F","staffNumber":"1 001–5 000 человек","registrationDate":"2018-04-20T08:00:04+00:00","representativeUser":null,"contacts":[{"title":"Сайт","url":"http:\u002F\u002Fwww.neoflex.ru\u002F","siteTitle":"","favicon":""},{"title":"Facebook","url":"https:\u002F\u002Ffacebook.com\u002Fneoflexcompany","siteTitle":"Войдите на Facebook","favicon":"https:\u002F\u002Fstatic.xx.fbcdn.net\u002Frsrc.php\u002Fyv\u002Fr\u002FB8BxsscfVBr.ico"},{"title":"Telegram","url":"https:\u002F\u002Ftelegram.me\u002Fneoflexlive","siteTitle":"Telegram: Contact @neoflexlive","favicon":"https:\u002F\u002Ftelegram.org\u002Fimg\u002Fwebsite_icon.svg?4"}],"settings":{"analyticsSettings":[{"type":"ga","trackingId":"UA-127726177-1"}],"branding":null,"status":"active","isStartup":false,"hasActivePolls":false},"metadata":{"titleHtml":"Neoflex, Москва - Создаем ИТ-платформы для цифровой трансформации с 21 февраля 2005 г.","title":"Neoflex, Москва - Создаем ИТ-платформы для цифровой трансформации с 21 февраля 2005 г.","keywords":["Big Data","Python","Управление персоналом","Apache","Карьера в IT-индустрии","neoflex","spark","hadoop","sql","bigdata","python","kafka","администрирование","cloudera","apache spark","управление проектами","monitoring","nifi","devops","data lake","data platform","kubernetes","dwh","data engineering","json"],"descriptionHtml":"52 статьи от авторов компании Neoflex","description":"52 статьи от авторов компании Neoflex"},"aDeskSettings":null,"careerAlias":"neoflex-ru"}},"companyIds":{},"companyTopIds":{},"pagesCount":{},"companyProfiles":{},"companiesCategories":[],"companiesCategoriesTotalCount":0,"companiesWidgets":{},"companiesWorkers":{},"companiesFans":{},"route":{},"isLoading":false,"companyWorkersLoading":false,"companyFansLoading":false,"vacancies":{},"companiesGalleries":{},"companiesBanners":{},"companiesLandingVacancies":{},"companiesTechnologies":{},"workplaceInfo":null},"companyAdmin":{"companyInfo":null,"companyInfoLoading":false,"faqArticles":null,"brandingPreviewImageUrl":null,"jivoStatus":0,"adminNotifications":null},"companiesContribution":{"hubs":{},"flows":{},"companyRefs":{}},"companyHubsContribution":{"contributionRefs":{"hubRefs":{},"hubIds":{}}},"conversation":{"messages":[],"respondent":null,"isLoadMore":false},"conversations":{"conversations":[],"unreadCount":0,"pagesCount":0},"desktopState":{"desktopFl":null,"desktopHl":null,"isChecked":false,"isLoginDemanded":false},"docs":{"menu":{},"articles":{},"mainMenu":[],"loading":{"main":false,"dropdown":false,"article":false}},"feature":{"isProbablyVisible":true},"flows":{"updates":{"countNewPostsBySubscription":null,"countNewPostsAll":67,"countNewNewsAll":54},"flows":[{"alias":"develop","id":"1","route":{"name":"FLOW_PAGE","params":{"flowName":"develop"}}},{"alias":"admin","id":"6","route":{"name":"FLOW_PAGE","params":{"flowName":"admin"}}},{"alias":"design","id":"2","route":{"name":"FLOW_PAGE","params":{"flowName":"design"}}},{"alias":"management","id":"3","route":{"name":"FLOW_PAGE","params":{"flowName":"management"}}},{"alias":"marketing","id":"4","route":{"name":"FLOW_PAGE","params":{"flowName":"marketing"}}},{"alias":"popsci","id":"7","route":{"name":"FLOW_PAGE","params":{"flowName":"popsci"}}}]},"global":{"isPwa":false,"device":"mobile","isHabrCom":true},"hubs":{"hubRefs":{},"hubIds":{},"pagesCount":{},"isLoading":false,"route":{}},"hubsBlock":{"hubRefs":{},"hubIds":{}},"i18n":{"fl":"ru","hl":"ru"},"info":{"infoPage":{},"isLoading":true},"location":{"urlStruct":{"protocol":null,"slashes":null,"auth":null,"host":null,"port":null,"hostname":null,"hash":null,"search":null,"query":{},"pathname":null,"path":null,"href":""}},"me":{"user":null,"uuid":null,"ppgDemanded":false,"karmaResetInfo":{"canReincarnate":null,"wasReincarnated":null,"currentScore":null},"notes":null},"modal":{"modals":[]},"mostReadingList":{"mostReadingListIds":[],"mostReadingListRefs":null,"promoPost":null},"ppa":{"articles":{},"card":null,"transactions":null,"totalTransactions":null,"isAccessible":null},"projectsBlocks":{"activeBlocks":{}},"promoData":{"isLoading":false,"hasLoaded":false,"featurer":null,"megaposts":null,"promoLinks":null,"promoPosts":null},"pullRefresh":{"shouldRefresh":false},"sandbox":{"articleIds":[],"articleRefs":{},"pagesCount":null,"route":{},"lastVisitedRoute":{},"isLoading":false},"search":{"searchQueryError":null},"settingsOther":{"inputs":{"uiLang":{"errors":[],"ref":null,"value":""},"articlesLangEnglish":{"errors":[],"ref":null,"value":false},"articlesLangRussian":{"errors":[],"ref":null,"value":false},"agreement":{"errors":[],"ref":null,"value":false},"email":{"errors":[],"ref":null,"value":true},"digest":{"errors":[],"ref":null,"value":true}}},"similarList":{"similarListIds":[],"similarListRefs":null},"ssr":{"error":null,"isDataLoaded":false,"isDataLoading":false,"isHydrationFailed":false,"isServer":false},"stories":{"stories":[{"id":"story-302","author":{"logo":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Ffa0\u002F939\u002F5d1\u002Ffa09395d14e24eccbc732d3224b88c35.png","title":"Хабр Карьера","link":"https:\u002F\u002Fu.habr.com\u002FiBejh"},"title":"Техническое собеседование тестировщика","lang":"ru","startTime":"2023-03-15T17:25:00+03:00","finishTime":"2023-03-22T17:18:24+03:00","slides":[{"id":"story-302_1","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fd3a\u002F834\u002F476\u002Fd3a834476c7f0ef78c446688b40bcf92.png","button":null},{"id":"story-302_2","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F37d\u002F2cd\u002F716\u002F37d2cd716ea3bcf76205f7b0e53cbfce.png","button":null},{"id":"story-302_3","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fb1b\u002Fe49\u002Fbca\u002Fb1be49bca3163216af7498c51df10d57.png","button":{"title":"Смотреть","link":"https:\u002F\u002Fu.habr.com\u002FiBejh","colorType":"dark"}}]},{"id":"story-300","author":{"logo":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F733\u002Ff4f\u002Fcfb\u002F733f4fcfb4ea40096d8b3b233eeabed7.png","title":"Хабр Новости","link":"https:\u002F\u002Fu.habr.com\u002FDaBkU"},"title":"Tesla Bot в Atomic Heart","lang":"ru","startTime":"2023-03-15T14:00:00+03:00","finishTime":"2023-03-17T23:59:00+03:00","slides":[{"id":"story-300_1","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F92b\u002F5db\u002Fa88\u002F92b5dba8866be4b38399b6c27b1c8f66.jpg","button":null},{"id":"story-300_2","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F1af\u002F4b9\u002F1fb\u002F1af4b91fba77623e6042130d7acc081b.jpg","button":null},{"id":"story-300_3","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F427\u002F695\u002F8f5\u002F4276958f56d2451cf11d1a0f3ed3c4a8.jpg","button":{"title":"Подробнее","link":"https:\u002F\u002Fu.habr.com\u002FDaBkU","colorType":"light"}}]},{"id":"story-298","author":{"logo":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F861\u002F278\u002F7b0\u002F8612787b0e2137d8cf487ce655a8bc60.png","title":"Хабр Новости","link":"https:\u002F\u002Fu.habr.com\u002FR9kas"},"title":"Робопсы патрулируют ЦОД в Цюрихе","lang":"ru","startTime":"2023-03-15T13:52:00+03:00","finishTime":"2023-03-17T22:30:00+03:00","slides":[{"id":"story-298_1","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F86c\u002F3e9\u002Fb24\u002F86c3e9b2404091d1675b4e82fbc670c2.jpg","button":null},{"id":"story-298_2","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F338\u002F37a\u002Ff4e\u002F33837af4e14c4502f1262273cff0be0b.jpg","button":null},{"id":"story-298_3","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Ffad\u002F29f\u002F78b\u002Ffad29f78bc03ca9d473d2da93629fa5f.jpg","button":{"title":"Подробнее","link":"https:\u002F\u002Fu.habr.com\u002FR9kas","colorType":"light"}}]},{"id":"story-296","author":{"logo":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fd59\u002Fa4b\u002Fc74\u002Fd59a4bc74deec559888728bac63c8744.png","title":"Хабр Карьера","link":"https:\u002F\u002Fu.habr.com\u002FfnoO9"},"title":"Кем работать в IT: Scrum-мастер","lang":"ru","startTime":"2023-03-14T11:10:00+03:00","finishTime":"2023-03-21T23:59:00+03:00","slides":[{"id":"story-296_1","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F50c\u002Ff4e\u002Fb0b\u002F50cf4eb0b63bacf94fe49737d0e00958.png","button":null},{"id":"story-296_2","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F4cf\u002F981\u002Fb95\u002F4cf981b95ef9afeb773d191612ddfec3.png","button":null},{"id":"story-296_3","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F3ab\u002Fe79\u002F5da\u002F3abe795da78dcd4c1f951a7eb6481608.png","button":{"title":"Подробнее","link":"https:\u002F\u002Fu.habr.com\u002FfnoO9","colorType":"dark"}},{"id":"story-296_4","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F72f\u002Fe3e\u002F46f\u002F72fe3e46faaf60ec518100b155b61d73.png","button":{"title":"Подробнее","link":"https:\u002F\u002Fu.habr.com\u002FfnoO9","colorType":"dark"}},{"id":"story-296_5","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F471\u002F060\u002F9a1\u002F4710609a1dfa6a508fef8ec62f031621.png","button":{"title":"Подробнее","link":"https:\u002F\u002Fu.habr.com\u002FfnoO9","colorType":"dark"}},{"id":"story-296_6","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F67f\u002F49b\u002F88c\u002F67f49b88c5e399b4d75c560288b72796.png","button":{"title":"Подробнее","link":"https:\u002F\u002Fu.habr.com\u002FfnoO9","colorType":"dark"}}]},{"id":"story-280","author":{"logo":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F8f3\u002Ff89\u002Faa5\u002F8f3f89aa500a5fc407f7e1e927002ce0.png","title":"Geek Travel","link":null},"title":"Где в России покататься на дрезине","lang":"ru","startTime":"2023-03-09T10:00:00+03:00","finishTime":"2023-03-19T23:59:00+03:00","slides":[{"id":"story-280_1","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fb44\u002F68b\u002Fd51\u002Fb4468bd51494723807a4438f03a9824d.jpg","button":null},{"id":"story-280_2","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F3aa\u002Fafe\u002F0fb\u002F3aaafe0fb803c0043aff70ae407a6da0.jpg","button":null},{"id":"story-280_3","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fca6\u002F8ed\u002F3d8\u002Fca68ed3d8251f5e25034d27c4e73da3a.jpg","button":null},{"id":"story-280_4","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Ff02\u002Ff05\u002Fafd\u002Ff02f05afda6f5925e4aaa3748953575c.jpg","button":null},{"id":"story-280_5","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fca9\u002Fa6a\u002Fad2\u002Fca9a6aad2113a292566924b015a44d88.jpg","button":null},{"id":"story-280_6","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F4f8\u002F708\u002Faa9\u002F4f8708aa9526b05dd8bc0e23dbbb59d7.jpg","button":null},{"id":"story-280_7","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F2af\u002F9d2\u002F852\u002F2af9d2852a460d69364e0969cbb7d04f.jpg","button":null}]},{"id":"story-288","author":{"logo":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F81b\u002F4f7\u002F1e8\u002F81b4f71e8fdc766f6d91491a5179ecb0.png","title":"Хабр Карьера","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Fusers\u002Fhabr_career\u002Fposts\u002F"},"title":"Разбор резюме специалистов на Хабр Карьере","lang":"ru","startTime":"2023-03-08T13:00:00+03:00","finishTime":"2023-03-15T23:59:00+03:00","slides":[{"id":"story-288_1","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F85a\u002F0ec\u002Fde0\u002F85a0ecde038a35f01ff328a3e67f5450.png","button":null},{"id":"story-288_2","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F885\u002F078\u002F3e1\u002F8850783e127c042f4db7a31cbe43c561.png","button":null},{"id":"story-288_3","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F876\u002F34c\u002F5b8\u002F87634c5b855ea18469a3024e386bb160.png","button":{"title":"Смотреть разбор","link":"https:\u002F\u002Fu.habr.com\u002FyDfoT","colorType":"dark"}}]},{"id":"story-278","author":{"logo":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F3b2\u002F2b9\u002F874\u002F3b22b9874c30d2e95f06e8ba591d7226.png","title":"Хабр","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Fcompany\u002Fhabr\u002Fblog\u002F"},"title":"Схватил за мозг","lang":"ru","startTime":"2023-03-06T11:00:00+03:00","finishTime":"2023-03-19T23:59:00+03:00","slides":[{"id":"story-278_1","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F6d2\u002F515\u002F649\u002F6d2515649a98ebff0fff2c95f13aa9e6.jpg","button":null},{"id":"story-278_2","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F6ba\u002F37c\u002F756\u002F6ba37c75671941bb212e0bf815f99550.jpg","button":{"title":"Что оказалось?","link":"https:\u002F\u002Fu.habr.com\u002Fbrain01","colorType":"light"}},{"id":"story-278_3","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F702\u002Fbe8\u002Fa8f\u002F702be8a8fe21d483936df54dc074b96a.jpg","button":{"title":"Осознать","link":"https:\u002F\u002Fu.habr.com\u002Fbrain02","colorType":"light"}},{"id":"story-278_4","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F9e2\u002F2c2\u002F499\u002F9e22c2499018c37dbb4064053ef7e0a0.jpg","button":{"title":"Отложить таблетку","link":"https:\u002F\u002Fu.habr.com\u002Fbrain03","colorType":"dark"}},{"id":"story-278_5","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F756\u002F85f\u002F99b\u002F75685f99b00f7a2323028948818a1b66.jpg","button":{"title":"Смотреть советы","link":"https:\u002F\u002Fu.habr.com\u002Fbrain04","colorType":"dark"}},{"id":"story-278_6","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F442\u002F9a8\u002F159\u002F4429a8159a4280575a30cf1bbbdb3b08.jpg","button":{"title":"Уделить внимание","link":"https:\u002F\u002Fu.habr.com\u002Fbrain05","colorType":"light"}},{"id":"story-278_7","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F892\u002Fe3e\u002Fa8b\u002F892e3ea8b4ad8934c6858348943804fd.jpg","button":{"title":"Записать нужное","link":"https:\u002F\u002Fu.habr.com\u002Fbrain06","colorType":"light"}},{"id":"story-278_8","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Ffa5\u002Ffe9\u002F7bb\u002Ffa5fe97bb1ec4af1e7b1e164171b795c.jpg","button":{"title":"Какое именно?","link":"https:\u002F\u002Fu.habr.com\u002Fbrain07","colorType":"light"}}]},{"id":"story-276","author":{"logo":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F371\u002F16e\u002F9ae\u002F37116e9aec24a99308b5d55fd3385062.png","title":"Хабр","link":"https:\u002F\u002Fu.habr.com\u002Fstories_habrapp"},"title":"Приложение Хабра для Android и iOS","lang":"ru","startTime":"2023-03-06T10:00:00+03:00","finishTime":"2023-03-19T23:59:00+03:00","slides":[{"id":"story-276_1","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F5d5\u002Fadb\u002F1dd\u002F5d5adb1dd2abf6a2e7f8956f4879eced.jpg","button":{"title":"Вот это новость!","link":"https:\u002F\u002Fu.habr.com\u002Fstories_habrapp","colorType":"light"}},{"id":"story-276_2","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F2e1\u002Fd75\u002F99c\u002F2e1d7599c38e3c11d124d5e8beece6cc.jpg","button":{"title":"А что у вас за стек?","link":"https:\u002F\u002Fu.habr.com\u002Fstories_habrapp","colorType":"dark"}},{"id":"story-276_3","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F448\u002Fe83\u002F1f9\u002F448e831f9da9256729dff47718aefdab.jpg","button":{"title":"Перейти к приложению","link":"https:\u002F\u002Fu.habr.com\u002FK8Ib8","colorType":"light"}},{"id":"story-276_4","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Faa0\u002F395\u002F080\u002Faa0395080db2f6eebbc22620a3153c1c.jpg","button":{"title":"Перейти к приложению","link":"https:\u002F\u002Fu.habr.com\u002FFatyV","colorType":"dark"}}]},{"id":"story-272","author":{"logo":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F82b\u002Fdd7\u002F7d8\u002F82bdd77d8ef7304d3b1b599191c2bb1c.png","title":"Сезоны Хабра","link":"https:\u002F\u002Fu.habr.com\u002F5P6i7"},"title":"Открыт сезон ML","lang":"ru","startTime":"2023-03-02T09:00:41+03:00","finishTime":"2023-03-31T20:53:00+03:00","slides":[{"id":"story-272_1","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F27f\u002Fd3a\u002Fddf\u002F27fd3addfe06da09592bdcf61db15795.jpg","button":null},{"id":"story-272_2","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F935\u002F582\u002F992\u002F935582992e378add33c898451b2a0d9f.jpg","button":null},{"id":"story-272_3","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fe13\u002Feb8\u002Fa24\u002Fe13eb8a24b60622d3da72fd6e7ed3fda.jpg","button":null},{"id":"story-272_4","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fa85\u002F83f\u002F5cc\u002Fa8583f5ccfae747135343f0e68c84112.jpg","button":{"title":"Подробнее","link":"https:\u002F\u002Fu.habr.com\u002F5P6i7","colorType":"light"}},{"id":"story-272_5","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F60d\u002F1cb\u002F98c\u002F60d1cb98cdd42d2696ebc0ec54740a23.jpg","button":{"title":"Подробнее","link":"https:\u002F\u002Fu.habr.com\u002F5P6i7","colorType":"light"}}]},{"id":"story-262","author":{"logo":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F2fa\u002Fc8a\u002Fba8\u002F2fac8aba82b995297abea595da7990d1.png","title":"Хабр","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Fcompany\u002Fhabr\u002Fblog\u002F"},"title":"Наука сна","lang":"ru","startTime":"2023-02-28T12:15:00+03:00","finishTime":"2023-03-15T23:59:00+03:00","slides":[{"id":"story-262_1","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fa90\u002F725\u002F61c\u002Fa9072561c280c46933ff54fd8702cade.jpg","button":null},{"id":"story-262_2","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Ffd7\u002F658\u002F956\u002Ffd7658956ea825b1c317c7f688c48058.jpg","button":{"title":"Читать лекцию","link":"https:\u002F\u002Fu.habr.com\u002Fson01","colorType":"light"}},{"id":"story-262_3","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F366\u002Fd2e\u002F707\u002F366d2e70725209177fa82a4119139c86.jpg","button":{"title":"При чём же?","link":"https:\u002F\u002Fu.habr.com\u002Fson02","colorType":"light"}},{"id":"story-262_4","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F4bd\u002F40f\u002F731\u002F4bd40f731bfbf571a709dd2f357f06b4.jpg","button":{"title":"Заглянуть в дневник","link":"https:\u002F\u002Fu.habr.com\u002Fson03","colorType":"light"}},{"id":"story-262_5","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F944\u002F7ad\u002Fe2f\u002F9447ade2f38c3c36790be75bf4462047.jpg","button":{"title":"Последствия","link":"https:\u002F\u002Fu.habr.com\u002Fson04","colorType":"light"}},{"id":"story-262_6","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F74c\u002Fce1\u002F4dc\u002F74cce14dc903b245dca90df3f2747abb.jpg","button":{"title":"Смотреть список","link":"https:\u002F\u002Fu.habr.com\u002Fson05","colorType":"light"}},{"id":"story-262_7","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fbc9\u002Fa62\u002F9f0\u002Fbc9a629f0c5ef2027cf7796749a06914.jpg","button":{"title":"Следы в истории","link":"https:\u002F\u002Fu.habr.com\u002Fson06","colorType":"light"}},{"id":"story-262_8","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F085\u002F0ee\u002F868\u002F0850ee868132e1ec4ec3b24caf7e719a.jpg","button":{"title":"Что придумали?","link":"https:\u002F\u002Fu.habr.com\u002Fson07","colorType":"light"}}]},{"id":"story-152","author":{"logo":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fa49\u002F0d3\u002F609\u002Fa490d3609f29b6dfc53a7b88ab47ca64.png","title":"Хабр","link":"https:\u002F\u002Fu.habr.com\u002Fstory_HiM-anketa"},"title":"Сеньоры — очень странные люди","lang":"ru","startTime":"2023-01-23T11:00:00+03:00","finishTime":"2023-03-31T23:59:00+03:00","slides":[{"id":"story-152_1","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fbbb\u002F901\u002F063\u002Fbbb9010630ae483ed1f41ec4e2572c13.png","button":null},{"id":"story-152_2","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F4f8\u002F66c\u002F14f\u002F4f866c14f0ca8999334438d7f17c0e3a.png","button":null},{"id":"story-152_3","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F6b9\u002Fa07\u002Fa61\u002F6b9a07a612df213ac3a1c179301828df.png","button":null},{"id":"story-152_4","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Ff6b\u002Fadb\u002Fb99\u002Ff6badbb9969d7e34ef4f7492767c6a77.png","button":null},{"id":"story-152_5","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fd9c\u002F9ac\u002F878\u002Fd9c9ac878bec5b494699e0080dd2e813.png","button":null},{"id":"story-152_6","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fdb1\u002Faf9\u002Fc3b\u002Fdb1af9c3bb557f6cee698cf67610f977.png","button":null},{"id":"story-152_7","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F5cf\u002Fbed\u002F65b\u002F5cfbed65bd20b7051b1aac2b9d6e807e.png","button":null},{"id":"story-152_8","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Ffa8\u002F251\u002F4a9\u002Ffa82514a9471cd455339e97178d8c021.png","button":null},{"id":"story-152_9","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fbd5\u002F901\u002Fe95\u002Fbd5901e952d9cf638355a3cc8b0c0911.png","button":{"title":"Хочу помочь джуну","link":"https:\u002F\u002Fu.habr.com\u002Fstory_HiM-anketa","colorType":"light"}}]}]},"technotext":{"years":[],"technotextDocForNominees":null,"technotextDocForWinners":null,"technotextInfo":{},"technotextInfoLoading":false,"technotextWinners":{},"technotextWinnersLoading":false},"userHubsContribution":{"contributionRefs":{"hubRefs":{},"hubIds":{}}},"userInvites":{"availableInvites":0,"usedInvitesIds":[],"usedInvitesRefs":{},"usedInvitesPagesCount":0,"unusedInvitesIds":[],"unusedInvitesRefs":{},"unusedInvitesPagesCount":0},"userVotes":{"karmaVotesList":[],"karmaVotesPagesCount":null,"karmaVotesListLoading":false,"commentsVotesList":[],"commentsVotesPagesCount":null,"commentsVotesListLoading":false,"postsVotesList":[],"postsVotesPagesCount":null,"postsVotesListLoading":false,"userVotesList":[],"userVotesPagesCount":null,"userVotesListLoading":false},"users":{"authorRefs":{},"authorIds":{},"pagesCount":{},"authorProfiles":{},"userHubs":{},"userInvitations":{},"authorFollowers":{},"authorFollowed":{},"userSpecialization":{},"karmaStats":[],"statistics":null,"isLoading":false,"authorFollowersLoading":false,"authorFollowedLoading":false,"userHubsLoading":false,"userInvitationsLoading":false,"route":{}},"viewport":{"prevScrollY":{},"scrollY":0,"width":0},"tracker":{"items":{},"pagesCache":{},"markedViewedSilently":{},"markedRead":{},"unreadCounters":{"applications":null,"system":null,"mentions":null,"subscribers":null,"posts_and_comments":null},"unviewedCounters":{"applications":null,"system":null,"mentions":null,"subscribers":null,"posts_and_comments":null}}};(function(){var s;(s=document.currentScript||document.scripts[document.scripts.length-1]).parentNode.removeChild(s);}());</script>
<script src="https://assets.habr.com/habr-web/js/chunk-vendors.76685433.js" defer></script><script src="https://assets.habr.com/habr-web/js/7298.c8f1d73c.js" defer></script><script src="https://assets.habr.com/habr-web/js/app.067cb56a.js" defer></script>



    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-726094-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
    </script>
  
  <script type="text/javascript" >
    (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
    m[i].l=1*new Date();k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
    (window, document, "script", "https://mc.yandex.ru/metrika/tag.js", "ym");

    ym(24049213, "init", {
      defer:true,
      trackLinks:true,
      accurateTrackBounce:true,
      webvisor:false,
    });
  </script>
  <noscript>
    <div>
      <img src="https://mc.yandex.ru/watch/24049213" style="position:absolute; left:-9999px;" alt="" />
    </div>
  </noscript>
  
    <script type="text/javascript">
      window.addEventListener('load', function () {
        setTimeout(() => {
          const img = new Image();
          img.src = 'https://vk.com/rtrg?p=VK-RTRG-421343-57vKE';
        }, 0);
      });
    </script>
  
<script src="/js/ads.js" onload="window['zhY4i4nJ9K'] = true"></script>
</body>
</html>

